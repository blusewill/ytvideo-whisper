{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blusewill/ytvideo-whisper/blob/dev/ytvideo_whisper_KMN_BOT_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QQL1GjFTNNp"
      },
      "source": [
        "# ytvideo-whisper 機器狼特別版\n",
        "\n",
        "這個專案採用了[Whisper](https://github.com/openai/whisper)而不是以前在用的Decipher專案\n",
        "\n",
        "如果想要用原本的Whisper專案，就在這裡"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrZvIN-oUKdx"
      },
      "source": [
        "# 使用方法\n",
        "\n",
        "1. 請確認你的執行階段有啟用GPU，在 ``執行階段 -> 變更執行階段類型 ``\n",
        "1. 更改設定\n",
        "1. 按下 ``執行階段 -> 執行全部`` 或是 (CTRL+F9)\n",
        "1. 然後連接到Google雲端硬碟\n",
        "1. 然後等一下你的檔案應該就會在 ``雲端硬碟 -> Whisper -> result``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTpjyRvaUVZh"
      },
      "source": [
        "## 獲得Google雲端硬碟權限\n",
        "(在 雲端硬碟 -> Whisper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-M2C9UlxTCjE"
      },
      "outputs": [],
      "source": [
        "#@markdown # **下載選項**\n",
        "#@markdown ---\n",
        "#@markdown ## **停用Google雲端硬碟**\n",
        "disable_google_drive = \"yes\" #@param ['yes', 'no']\n",
        "# @markdown ## **下載方式** ``anonfile / transfer.sh / 直線下載``\n",
        "download_method = \"transfer.sh\" #@param ['anonfile', 'transfer.sh', 'direct']\n",
        "#@markdown 當啟用 ``結束自動刪除執行階段`` 的時候**無法**使用直線下載，會回到上傳到anonfiles的方式。請見諒 </p>\n",
        "#@markdown **警告：直線下載有時候無法在Firefox或類Firefox瀏覽器 (WaterFox , Librewolf)使用**。如果無法使用的話，請**切換到上傳檔案到anonfile**。\n",
        "if disable_google_drive.lower() == 'no':\n",
        "\n",
        "  from google.colab import drive\n",
        "  import os\n",
        "\n",
        "  path = '/content/gdrive/MyDrive/Whisper/'\n",
        "  path2 = '/content/gdrive/MyDrive/Whisper/result'\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  if not os.path.exists(path):\n",
        "    os.mkdir(path)\n",
        "    os.mkdir(path2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiME-JOSUfhd"
      },
      "source": [
        "# 安裝程式\n",
        "openai-whisper yt-dlp ipython transfersh_client pyperclip wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aXCqLjY0Uz0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe1cce3-ae24-42e8-c808-b0bf02129aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20230314.tar.gz (792 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.9/792.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2023.7.6-py2.py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (7.34.0)\n",
            "Collecting transfersh_client\n",
            "  Downloading transfersh_client-1.1.3-py2.py3-none-any.whl (6.3 kB)\n",
            "Collecting pyperclip\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.65.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (9.1.0)\n",
            "Collecting tiktoken==0.3.1 (from openai-whisper)\n",
            "  Downloading tiktoken-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0 (from openai-whisper)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python==0.2.0->openai-whisper) (0.18.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.1->openai-whisper) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.1->openai-whisper) (2.27.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper) (3.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper) (3.12.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper) (16.0.6)\n",
            "Collecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.46.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets (from yt-dlp)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2023.7.22)\n",
            "Collecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.6)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.39.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (1.26.16)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper, pyperclip, wget\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=796908 sha256=da4e7152a6eb5f7d4e17a215e193228c226f62b9c18b22c2b0462039c92ae1c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/13/5f/fe8245f6dc59df505879da4b2129932e342f02a80e6b87f27d\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11123 sha256=d8e85b4617af630304dc3124085d4552ce0cbd99ed40db3b97338eae7cc07fbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/24/fe/140a94a7f1036003ede94579e6b4227fe96c840c6f4dcbe307\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=ce5f263791475374b00883c328d36f7f39e32ba5b581673e6390b2e0c202cd6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built openai-whisper pyperclip wget\n",
            "Installing collected packages: wget, transfersh_client, pyperclip, brotli, websockets, pycryptodomex, mutagen, jedi, ffmpeg-python, yt-dlp, tiktoken, openai-whisper\n",
            "Successfully installed brotli-1.0.9 ffmpeg-python-0.2.0 jedi-0.19.0 mutagen-1.46.0 openai-whisper-20230314 pycryptodomex-3.18.0 pyperclip-1.8.2 tiktoken-0.3.1 transfersh_client-1.1.3 websockets-11.0.3 wget-3.2 yt-dlp-2023.7.6\n"
          ]
        }
      ],
      "source": [
        "! apt-get install ffmpeg\n",
        "! pip install openai-whisper yt-dlp ipython transfersh_client pyperclip wget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWfd0B3nU_5o"
      },
      "source": [
        "# 測試GPU是否有開啟\n",
        "\n",
        "如果沒有東西跑出來的話，請去 ``執行階段 -> 變更執行階段類型`` 更改"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ukF_tYc9VfGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10454cad-2039-4857-80aa-9b4504a46d25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Aug  3 07:28:39 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRpdHsXmVqsv"
      },
      "source": [
        "# 設定\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IccTHdDOVxgL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#@markdown # **設定**\n",
        "#@markdown ---\n",
        "#@markdown ## **影片連接**\n",
        "# YouTube Link\n",
        "YouTube_Video_Link = \"https://www.youtube.com/watch?v=FbyV0TwL-wg\" #@param {type:\"string\"}\n",
        "# File Name Change\n",
        "#@markdown ## **檔案名稱**\n",
        "new_filename = \"How to make X to be Twitter\" #@param {type:\"string\"}\n",
        "#@markdown ## **語言**\n",
        "language = \"English\" #@param [\"Auto\", \"Chinese\", \"English\", \"Japanese\"]\n",
        "#@markdown ## **模式 ``生成 / 翻譯``**\n",
        "task = \"transcribe\" #@param ['transcribe', 'translate']\n",
        "# Change it to srt file type\n",
        "new_filename = os.path.splitext(new_filename)[0] + \".srt\"\n",
        "new_filename2 = os.path.splitext(new_filename)[0] + \" Transcript.txt\"\n",
        "# Change the Model of the Whisper\n",
        "#@markdown # **其他選項**\n",
        "#@markdown ---\n",
        "#@markdown ## **生產逐字稿**\n",
        "Generate_Plain_Document = \"yes\" #@param ['yes','no']\n",
        "#@markdown ## **結束自動刪除執行階段**\n",
        "shutdown_after_complete = \"yes\" #@param ['yes','no']\n",
        "#@markdown ## **模型**\n",
        "model_user = \"medium\" #@param ['tiny.en','tiny','base.en','base','small.en','small','medium.en','medium','large-v1','large-v2','large']\n",
        "#@markdown ## **Cookies驗證**\n",
        "enable_cookies = \"no\" #@param ['yes', 'no']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 如果你有啟動Cookies驗證\n",
        "請在這裡上傳Cookies檔案"
      ],
      "metadata": {
        "id": "MQa-cRGkAopn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if enable_cookies.lower() == \"yes\":\n",
        "  print(\"It is not recommended to upload your main account's cookies files here, as this is a shared environment.\")\n",
        "  print(\"Your cookies might get leaked, and I will not take any responsibility if your account gets stolen.\")\n",
        "  print(\"The recommended approach is to use another Google Account's cookies to run this tool.\")\n",
        "  print(\"If it is a private video, you can share the video with that Google Account to grant access to it.\")\n",
        "  print(\"\")\n",
        "\n",
        "  from google.colab import files\n",
        "  cookies = files.upload()\n",
        "\n",
        "  # Specify the path where the files are located\n",
        "  path_cookies = \"/content\"\n",
        "\n",
        "  def rename_files(path_cookies):\n",
        "      for filename in os.listdir(path_cookies):\n",
        "          file_path = os.path.join(path_cookies, filename)\n",
        "          if os.path.isfile(file_path) and filename.endswith(\".txt\"):\n",
        "              new_path = os.path.join(path_cookies, \"cookies.txt\")\n",
        "              os.rename(file_path, new_path)\n",
        "\n",
        "  rename_files(path_cookies)"
      ],
      "metadata": {
        "id": "hPMZ1BQLAoY9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1vEu7rbV6xJ"
      },
      "source": [
        "# 下載影片\n",
        "使用yt-dlp跟ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o6o0YbciV_aS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "321a8d79-e5de-4fb0-ba32-0520b240a355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=FbyV0TwL-wg\n",
            "[youtube] FbyV0TwL-wg: Downloading webpage\n",
            "[youtube] FbyV0TwL-wg: Downloading ios player API JSON\n",
            "[youtube] FbyV0TwL-wg: Downloading android player API JSON\n",
            "[youtube] FbyV0TwL-wg: Downloading player 2363d0d2\n",
            "[youtube] FbyV0TwL-wg: Downloading m3u8 information\n",
            "[info] FbyV0TwL-wg: Downloading 1 format(s): 251\n",
            "[download] Destination: content/audio\n",
            "[download] 100% of    2.76MiB in 00:00:00 at 17.12MiB/s  \n",
            "[ExtractAudio] Destination: content/audio.mp3\n",
            "Deleting original file content/audio (pass -k to keep)\n"
          ]
        }
      ],
      "source": [
        "import yt_dlp\n",
        "\n",
        "if enable_cookies.lower() == \"yes\":\n",
        "\n",
        "  ydl_opts = {\n",
        "      'format': 'bestaudio/best',\n",
        "      'postprocessors': [{\n",
        "          'key': 'FFmpegExtractAudio',\n",
        "          'preferredcodec': 'mp3',\n",
        "          'preferredquality': '192',\n",
        "      }],\n",
        "          'outtmpl': 'content/audio',\n",
        "          'cookiefile': 'cookies.txt'\n",
        "  }\n",
        "\n",
        "else:\n",
        "\n",
        "    ydl_opts = {\n",
        "      'format': 'bestaudio/best',\n",
        "      'postprocessors': [{\n",
        "          'key': 'FFmpegExtractAudio',\n",
        "          'preferredcodec': 'mp3',\n",
        "          'preferredquality': '192',\n",
        "      }],\n",
        "          'outtmpl': 'content/audio',\n",
        "  }\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([YouTube_Video_Link])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzO86nhoWPu2"
      },
      "source": [
        "# 生成字幕檔"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZnFM1sy3WTBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58026448-3aa4-4663-ca1c-f2cd4a1b6627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:17<00:00, 88.0MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whisper model loaded.\n",
            "[00:00.000 --> 00:05.280]  Today, we are looking at X.\n",
            "[00:05.280 --> 00:12.960]  And this is the social media, originally called Twitter, but now it's called X.\n",
            "[00:12.960 --> 00:17.080]  Which have a nice domain called X.com.\n",
            "[00:17.080 --> 00:20.640]  The most easy domain that I can remember is in my brain.\n",
            "[00:20.640 --> 00:25.680]  So as we can see, this is our X.\n",
            "[00:25.680 --> 00:30.680]  And it don't have a nice blue bird that we all remember.\n",
            "[00:30.680 --> 00:33.360]  But what can I do?\n",
            "[00:33.360 --> 00:36.280]  Is enable this extension right here.\n",
            "[00:36.280 --> 00:38.480]  And I could refresh.\n",
            "[00:38.480 --> 00:39.960]  Everything is back to Twitter.\n",
            "[00:39.960 --> 00:42.040]  Our blue bird is back to life.\n",
            "[00:42.040 --> 00:46.200]  Now, a W says, how can you install this extension?\n",
            "[00:46.200 --> 00:51.880]  First thing, if you are a Firefox user, well, you can already go to Firefox marketplace.\n",
            "[00:51.880 --> 00:53.840]  You can already get this extension.\n",
            "[00:53.880 --> 00:56.320]  I'll put the link in description.\n",
            "[00:56.320 --> 00:58.240]  So you can go ahead and get it.\n",
            "[00:58.240 --> 01:03.480]  But you found Chromium based browser, like up here is on Google Chromium.\n",
            "[01:03.480 --> 01:06.720]  And you will not find it on web store.\n",
            "[01:06.720 --> 01:09.240]  You have to install it from source.\n",
            "[01:09.240 --> 01:12.520]  So what you have to do is go to a source right here.\n",
            "[01:12.520 --> 01:16.720]  This project is called XBGone, which you can see right here.\n",
            "[01:16.720 --> 01:20.160]  And you can see XBGone, the Firefox and Chrome extension.\n",
            "[01:20.200 --> 01:24.080]  If you are going to use Firefox, of course, I will give you a link.\n",
            "[01:24.080 --> 01:26.120]  And now we have to install it.\n",
            "[01:26.120 --> 01:28.360]  Installation comes so very easy.\n",
            "[01:28.360 --> 01:33.760]  We just go code, download zip, and put download zip right there.\n",
            "[01:33.760 --> 01:36.120]  Let's go XBGone master.\n",
            "[01:36.120 --> 01:38.960]  And we just open it, extract.\n",
            "[01:38.960 --> 01:42.120]  And we extract into here.\n",
            "[01:42.120 --> 01:45.800]  And now I have to overwrite all the file because I already have these files.\n",
            "[01:45.840 --> 01:53.520]  So now what you have to do is go to extension tab, add three dots, more tools, extensions.\n",
            "[01:53.520 --> 01:57.480]  And next you have to do is enable your developer mode.\n",
            "[01:57.480 --> 02:00.000]  Just right here, right up here.\n",
            "[02:00.000 --> 02:03.160]  And click load, unpacked.\n",
            "[02:03.160 --> 02:08.240]  And click downloads, and find the folder you just extracted.\n",
            "[02:08.240 --> 02:10.280]  It's called XBGone master.\n",
            "[02:10.280 --> 02:14.000]  And now you have to select it and it will be installed.\n",
            "[02:14.040 --> 02:22.680]  Now when you go back to Twitter, it should be Twitter instead of X.\n",
            "[02:22.680 --> 02:27.040]  But we have one problem here is not working on tweedec.\n",
            "[02:27.040 --> 02:29.600]  So you can see it says XPro here.\n",
            "[02:29.600 --> 02:31.920]  They update their name, I guess.\n",
            "[02:31.920 --> 02:33.800]  Origin is called tweedec preview.\n",
            "[02:33.800 --> 02:34.920]  Not XPro.\n",
            "[02:34.920 --> 02:39.880]  So I might fix this if I want to make a PR for them.\n",
            "[02:39.920 --> 02:44.280]  But you know, it's like 1.59am right now.\n",
            "[02:44.280 --> 02:47.760]  I would have some more thing to do later.\n",
            "[02:47.760 --> 02:50.600]  And I have to go sleep to feed my pocket mouse.\n",
            "[02:50.600 --> 03:01.280]  So that's just a short video talking about how you can revert back from X to Twitter.\n",
            "[03:01.280 --> 03:02.360]  Let me show you guys again.\n",
            "[03:02.360 --> 03:04.560]  It's Twitter, not X.\n",
            "[03:04.560 --> 03:05.400]  Just like that.\n",
            "[03:05.400 --> 03:07.000]  And I will see you guys next time.\n",
            "[03:07.000 --> 03:07.520]  Bye bye.\n",
            "Generated srt and txt file in the Google Drive -> Whisper -> result Folder\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(model_user)\n",
        "\n",
        "print(\"Whisper model loaded.\")\n",
        "\n",
        "\n",
        "from whisper.utils import get_writer\n",
        "\n",
        "filename = \"audio.mp3\"\n",
        "input_directory = \"content\"\n",
        "input_file = f\"{input_directory}/{filename}\"\n",
        "if language.lower() == 'Auto':\n",
        "  result = model.transcribe(input_file, verbose=True, task=task)\n",
        "else:\n",
        "  result = model.transcribe(input_file, verbose=True, task=task, language=language)\n",
        "\n",
        "if disable_google_drive.lower() == 'no':\n",
        "\n",
        "  transcription_root = \"/content/gdrive/MyDrive/Whisper/result\"\n",
        "\n",
        "else:\n",
        "  os.mkdir(\"result\")\n",
        "  transcription_root = \"/content/result\"\n",
        "\n",
        "# Save as an SRT file\n",
        "srt_writer = get_writer(\"srt\", transcription_root,)\n",
        "srt_writer(result, new_filename)\n",
        "\n",
        "if Generate_Plain_Document.lower() == \"yes\":\n",
        "\n",
        "  # Save as TXT file\n",
        "  srt_writer = get_writer(\"txt\", transcription_root,)\n",
        "  srt_writer(result, new_filename2)\n",
        "  print(\"Generated srt and txt file in the Google Drive -> Whisper -> result Folder\")\n",
        "\n",
        "else:\n",
        "\n",
        "  print(\"Generated srt file in the Google Drive -> Whisper -> result Folder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 壓縮檔案並上傳\n",
        "如果雲端硬碟被停用了"
      ],
      "metadata": {
        "id": "7rWCkivc2Jhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os.path\n",
        "import requests\n",
        "from google.colab import files\n",
        "from transfersh_client.app import send_to_transfersh, create_zip, remove_file\n",
        "\n",
        "if disable_google_drive.lower() == 'yes':\n",
        "    # Creating the ZIP file\n",
        "    archived = shutil.make_archive(new_filename, 'zip', '/content/result')\n",
        "\n",
        "    if os.path.exists(archived):\n",
        "        print(archived)\n",
        "\n",
        "        if shutdown_after_complete.lower() == \"yes\":\n",
        "            if download_method.lower() == \"anonfiles\":\n",
        "              # Upload the file to Anonfiles\n",
        "              upload_url = \"https://api.anonfiles.com/upload\"\n",
        "              with open(archived, 'rb') as file:\n",
        "                  response = requests.post(upload_url, files={'file': file})\n",
        "\n",
        "              # Check if the upload was successful\n",
        "              if response.status_code == 200 and response.json()['status']:\n",
        "                  download_url = response.json()['data']['file']['url']['full']\n",
        "                  print(f\"File uploaded successfully to Anonfiles. Download link: {download_url}\")\n",
        "              else:\n",
        "                  print(\"File upload to Anonfiles failed.\")\n",
        "            else:\n",
        "              # Upload to transfer.sh\n",
        "              def send_files_from_dir():\n",
        "                directory = '/content'\n",
        "                send_to_transfersh(archived, clipboard=False)  # sends archive to transfer.sh\n",
        "\n",
        "\n",
        "              if __name__ == '__main__':\n",
        "                send_files_from_dir()\n",
        "\n",
        "\n",
        "        elif download_method.lower() == \"anonfile\":\n",
        "            # Upload the file to Anonfiles\n",
        "            upload_url = \"https://api.anonfiles.com/upload\"\n",
        "            with open(archived, 'rb') as file:\n",
        "                response = requests.post(upload_url, files={'file': file})\n",
        "\n",
        "            # Check if the upload was successful\n",
        "            if response.status_code == 200 and response.json()['status']:\n",
        "                download_url = response.json()['data']['file']['url']['full']\n",
        "                print(f\"File uploaded successfully to Anonfiles. Download link: {download_url}\")\n",
        "            else:\n",
        "                print(\"File upload to Anonfiles failed.\")\n",
        "\n",
        "        elif download_method.lower() == \"transfer.sh\":\n",
        "            # Upload to transfer.sh\n",
        "            def send_files_from_dir():\n",
        "              directory = '/content'\n",
        "              send_to_transfersh(archived, clipboard=False)  # sends archive to transfer.sh\n",
        "\n",
        "\n",
        "            if __name__ == '__main__':\n",
        "              send_files_from_dir()\n",
        "\n",
        "        else:\n",
        "            files.download(archived)\n",
        "            print(\"File downloaded successfully.\")\n",
        "\n",
        "    else:\n",
        "        print(\"ZIP file not created\")"
      ],
      "metadata": {
        "id": "H-PkcMx22JTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db9995f-95ac-4e91-9e18-d87f3afc5da8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/How to make X to be Twitter.srt.zip\n",
            "\n",
            "Sending file: How to make X to be Twitter.srt.zip (size of the file: 0.003002 MB)\n",
            "Link to download file(will be saved till 2023-08-17):\n",
            "https://transfer.sh/qy0Izsb0Ai/How%20to%20make%20X%20to%20be%20Twitter.srt.zip\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM61x2JuYBl-"
      },
      "source": [
        "# 自動關閉執行階段"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUKHF0-rYGvq"
      },
      "outputs": [],
      "source": [
        "if shutdown_after_complete.lower() == \"yes\":\n",
        "    print(\"Shutting Down the Runtime Because Shutdown After Complete is on\")\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HiME-JOSUfhd"
      ],
      "provenance": [],
      "authorship_tag": "ABX9TyNUsSehGk7csStP5AOJgxjg",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}