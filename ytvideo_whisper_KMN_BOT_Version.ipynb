{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blusewill/ytvideo-whisper/blob/dev/ytvideo_whisper_KMN_BOT_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QQL1GjFTNNp"
      },
      "source": [
        "# ytvideo-whisper 機器狼特別版\n",
        "\n",
        "這個專案採用了[Whisper](https://github.com/openai/whisper)而不是以前在用的Decipher專案\n",
        "\n",
        "如果想要用原本的Whisper專案，就在這裡"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrZvIN-oUKdx"
      },
      "source": [
        "# 使用方法\n",
        "\n",
        "1. 請確認你的執行階段有啟用GPU，在 ``執行階段 -> 變更執行階段類型 ``\n",
        "1. 更改設定\n",
        "1. 按下 ``執行階段 -> 執行全部`` 或是 (CTRL+F9)\n",
        "1. 然後連接到Google雲端硬碟\n",
        "1. 然後等一下你的檔案應該就會在 ``雲端硬碟 -> Whisper -> result``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTpjyRvaUVZh"
      },
      "source": [
        "## 獲得Google雲端硬碟權限\n",
        "(在 雲端硬碟 -> Whisper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-M2C9UlxTCjE",
        "outputId": "161b0b01-ad1a-4da5-91c0-0950c5f43086",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **下載選項**\n",
        "#@markdown ---\n",
        "#@markdown ## **停用Google雲端硬碟**\n",
        "disable_google_drive = \"no\" #@param ['yes', 'no']\n",
        "# @markdown ## **下載方式** ``bashupload / 直線下載``\n",
        "download_method = \"bashupload\" #@param ['bashupload', 'direct']\n",
        "# @markdown 當啟用 ``結束自動刪除執行階段`` 的時候**無法**使用直線下載，會回到上傳到bashupload的方式。請見諒 </p>\n",
        "# @markdown **警告：直線下載有時候無法在Firefox或類Firefox瀏覽器 (WaterFox , Librewolf)使用**。如果無法使用的話，請**切換到上傳檔案到bashupload**。\n",
        "if disable_google_drive.lower() == 'no':\n",
        "\n",
        "  from google.colab import drive\n",
        "  import os\n",
        "\n",
        "  path = '/content/gdrive/MyDrive/Whisper/'\n",
        "  path2 = '/content/gdrive/MyDrive/Whisper/result'\n",
        "  drive.mount('/content/gdrive')\n",
        "\n",
        "  if not os.path.exists(path):\n",
        "    os.mkdir(path)\n",
        "    os.mkdir(path2)\n",
        "\n",
        "# Calcuate the Execution Time.\n",
        "from datetime import datetime\n",
        "start_time = datetime.now()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiME-JOSUfhd"
      },
      "source": [
        "# 安裝程式\n",
        "openai-whisper yt-dlp ipython pyperclip wget pycurl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aXCqLjY0Uz0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ff8c18-752e-405c-fa85-f82c9f3b01b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2024.5.26-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (7.34.0)\n",
            "Requirement already satisfied: pyperclip in /usr/local/lib/python3.10/dist-packages (1.8.2)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycurl\n",
            "  Downloading pycurl-7.45.3-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli (from yt-dlp)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2024.2.2)\n",
            "Collecting mutagen (from yt-dlp)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex (from yt-dlp)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.17 in /usr/local/lib/python3.10/dist-packages (from yt-dlp) (2.0.7)\n",
            "Collecting websockets>=12.0 (from yt-dlp)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.31.0->yt-dlp) (3.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.14.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper, wget\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=32639b16c9463a8f42d7468dbdc689a8c546503c90b4e321f92cd44bd52c057a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=d88f3dcac93b7cfc6a7db4659870bdba42d66efbbb024fcafe64ab219298015d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built openai-whisper wget\n",
            "Installing collected packages: wget, brotli, websockets, pycurl, pycryptodomex, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mutagen, jedi, yt-dlp, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed brotli-1.1.0 jedi-0.19.1 mutagen-1.47.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 pycryptodomex-3.20.0 pycurl-7.45.3 tiktoken-0.7.0 websockets-12.0 wget-3.2 yt-dlp-2024.5.26\n"
          ]
        }
      ],
      "source": [
        "! apt-get install ffmpeg\n",
        "! pip install openai-whisper yt-dlp ipython pyperclip wget pycurl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWfd0B3nU_5o"
      },
      "source": [
        "# 測試GPU是否有開啟\n",
        "\n",
        "如果沒有東西跑出來的話，請去 ``執行階段 -> 變更執行階段類型`` 更改"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ukF_tYc9VfGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d145081-7139-4088-bd39-024b91e90440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 27 16:56:50 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRpdHsXmVqsv"
      },
      "source": [
        "# 設定\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IccTHdDOVxgL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#@markdown # **設定 (星號代表基礎設定)**\n",
        "#@markdown ---\n",
        "#@markdown ## **上傳自己的音檔 (目前只限 mp3 檔)**\n",
        "upload_own_audio_file = \"no\" #@param ['yes', 'no']\n",
        "#@markdown ## **YouTube 模式**\n",
        "#@markdown ---\n",
        "#@markdown ## ***影片連接**\n",
        "# YouTube Link\n",
        "YouTube_Video_Link = \"https://www.youtube.com/watch?v=CUzY1lUTnec\" #@param {type:\"string\"}\n",
        "# File Name Change\n",
        "#@markdown ## ***檔案名稱**\n",
        "new_filename = \"Infurnity2023「ライフワンダーズスペシャルステージイベント」\" #@param {type:\"string\"}\n",
        "#@markdown ## ***語言 (自動，中文，英文，日文)**\n",
        "language = \"Auto\" #@param [\"Auto\", \"Chinese\", \"English\", \"Japanese\"]\n",
        "#@markdown ## ***模式 ``生成 / 翻譯``**\n",
        "task = \"transcribe\" #@param ['transcribe', 'translate']\n",
        "# Change it to srt file type\n",
        "new_filename = os.path.splitext(new_filename)[0] + \".srt\"\n",
        "new_filename2 = os.path.splitext(new_filename)[0] + \" Transcript.txt\"\n",
        "# Change the Model of the Whisper\n",
        "#@markdown # **其他選項**\n",
        "#@markdown ---\n",
        "#@markdown ## **生產逐字稿**\n",
        "Generate_Plain_Document = \"yes\" #@param ['yes','no']\n",
        "#@markdown ## **結束自動刪除執行階段**\n",
        "shutdown_after_complete = \"yes\" #@param ['yes','no']\n",
        "#@markdown ## **模型**\n",
        "model_user = \"medium\" #@param ['tiny.en','tiny','base.en','base','small.en','small','medium.en','medium','large-v1','large-v2','large']\n",
        "#@markdown ## **Cookies驗證**\n",
        "enable_cookies = \"no\" #@param ['yes', 'no']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 如果你有啟動Cookies驗證\n",
        "請在這裡上傳Cookies檔案"
      ],
      "metadata": {
        "id": "MQa-cRGkAopn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if enable_cookies.lower() == \"yes\":\n",
        "  print(\"It is not recommended to upload your main account's cookies files here, as this is a shared environment.\")\n",
        "  print(\"Your cookies might get leaked, and I will not take any responsibility if your account gets stolen.\")\n",
        "  print(\"The recommended approach is to use another Google Account's cookies to run this tool.\")\n",
        "  print(\"If it is a private video, you can share the video with that Google Account to grant access to it.\")\n",
        "  print(\"\")\n",
        "\n",
        "  from google.colab import files\n",
        "  cookies = files.upload()\n",
        "\n",
        "  # Specify the path where the files are located\n",
        "  path_cookies = \"/content\"\n",
        "\n",
        "  def rename_files(path_cookies):\n",
        "      for filename in os.listdir(path_cookies):\n",
        "          file_path = os.path.join(path_cookies, filename)\n",
        "          if os.path.isfile(file_path) and filename.endswith(\".txt\"):\n",
        "              new_path = os.path.join(path_cookies, \"cookies.txt\")\n",
        "              os.rename(file_path, new_path)\n",
        "\n",
        "  rename_files(path_cookies)"
      ],
      "metadata": {
        "id": "hPMZ1BQLAoY9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1vEu7rbV6xJ"
      },
      "source": [
        "# 下載影片/上傳音檔\n",
        "使用yt-dlp跟ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o6o0YbciV_aS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "34fca155-cfd2-4048-ae64-097d70c17a5f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cf19bbf1-33cd-4787-84e7-69e5127fbc39\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cf19bbf1-33cd-4787-84e7-69e5127fbc39\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 苹果M4性能分析：尽力了，但芯片工艺快到头了！ [EbDPvcbilCs].mp3 to 苹果M4性能分析：尽力了，但芯片工艺快到头了！ [EbDPvcbilCs].mp3\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "import yt_dlp\n",
        "\n",
        "if upload_own_audio_file == \"yes\":\n",
        "  if not os.path.isfile('audio.mp3'):\n",
        "      own_audio = files.upload()\n",
        "      if not os.path.exists('content'):\n",
        "        os.mkdir('content')\n",
        "        for filename in own_audio.keys():\n",
        "            os.rename(filename, 'audio.mp3')\n",
        "            os.replace('audio.mp3', 'content/audio.mp3')\n",
        "\n",
        "else:\n",
        "    if enable_cookies.lower() == \"yes\":\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "            'outtmpl': 'content/audio',\n",
        "            'cookiefile': 'cookies.txt'\n",
        "        }\n",
        "    else:\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "            'outtmpl': 'content/audio',\n",
        "        }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([YouTube_Video_Link])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzO86nhoWPu2"
      },
      "source": [
        "# 生成字幕檔"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZnFM1sy3WTBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b47942-7a38-4e96-83cb-12e77c6a2fb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:17<00:00, 88.1MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whisper model loaded.\n",
            "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
            "Detected language: Chinese\n",
            "[00:00.000 --> 00:03.040] 没想到 M3 才刚刚推出半年\n",
            "[00:03.040 --> 00:05.000] 这个月 M4 就已经来了\n",
            "[00:05.000 --> 00:06.080] 而且最离谱的是\n",
            "[00:06.080 --> 00:08.200] 它居然是首发在 iPad 上\n",
            "[00:08.200 --> 00:11.400] 我本来以为作为一个仅仅相隔半年推出的新处理器\n",
            "[00:11.400 --> 00:15.040] 它或许只是 M3 加强 NPU 换上 N3E 工艺的版本\n",
            "[00:15.040 --> 00:16.920] 实际上经过这几天的测试\n",
            "[00:16.920 --> 00:18.520] M4 已经向我们证明了\n",
            "[00:18.520 --> 00:20.360] 它是值得拥有一个新名字的\n",
            "[00:20.360 --> 00:23.520] 虽然它现在还禁锢在 iPad Pro 的小身板里\n",
            "[00:23.520 --> 00:27.960] 但这并不影响我们通过各种方法了解它的架构、性能和能效\n",
            "[00:28.040 --> 00:31.520] 这次测试 M4 我其实主要想了解两件事情\n",
            "[00:31.520 --> 00:33.360] 一是通过测试它的新架构\n",
            "[00:33.360 --> 00:36.080] 来看看苹果今年的性能上限如何\n",
            "[00:36.080 --> 00:39.640] 包括未来的新 Mac 甚至新的 iPhone 有多大的潜力\n",
            "[00:39.640 --> 00:42.800] 第二是作为全世界首颗 N3E 工艺的芯片\n",
            "[00:42.800 --> 00:44.480] 它的能耗表现如何\n",
            "[00:44.480 --> 00:45.520] 因为今年年底\n",
            "[00:45.520 --> 00:48.920] 各家的手机芯片都会使用 N3E 工艺来制造\n",
            "[00:48.920 --> 00:53.320] 这也可以从侧面为今年年底的手机芯片进步作一个预期\n",
            "[00:53.320 --> 00:55.840] 今天我们为大家带来了大量的实测\n",
            "[00:55.840 --> 00:58.520] 不仅会详细解析 M4 的架构和新特性\n",
            "[00:58.520 --> 01:01.040] 还有详细的能耗表现、基准测试\n",
            "[01:01.040 --> 01:05.680] 以及搭载 M4 芯片的 iPad 的游戏性能和续航表现\n",
            "[01:05.680 --> 01:08.440] 这期视频的信息量绝对是爆炸的\n",
            "[01:08.440 --> 01:09.040] 相信我\n",
            "[01:09.040 --> 01:10.600] 好饭绝对不怕晚\n",
            "[01:10.600 --> 01:11.760] 那么话不多说\n",
            "[01:11.760 --> 01:13.120] 咱们直接开始\n",
            "[01:13.120 --> 01:16.680] 首先先来跟我简单了解一下新的 M4 到底是个啥玩意儿\n",
            "[01:16.680 --> 01:18.320] 苹果在发布会上说\n",
            "[01:18.320 --> 01:22.320] 这颗处理器用的是台积电的第二代 3nm 工艺制造\n",
            "[01:22.320 --> 01:24.280] 其实这就是指 N3E 工艺\n",
            "[01:24.320 --> 01:27.240] 它也成为了第一颗使用 N3E 工艺制造的处理器\n",
            "[01:27.240 --> 01:28.200] 理论上来说\n",
            "[01:28.200 --> 01:32.640] 应该会比 M3 使用的 N3B 工艺有更高的性能上限\n",
            "[01:32.640 --> 01:33.560] CPU 部分\n",
            "[01:33.560 --> 01:36.920] 苹果在发布会上再次提到了大核的架构改进\n",
            "[01:36.920 --> 01:40.200] 我本来以为只是相比较于 M2 进一步加宽了\n",
            "[01:40.200 --> 01:42.040] 八成就是 M3 的马甲吧\n",
            "[01:42.040 --> 01:44.200] 然后后面的架构分析你就会看到\n",
            "[01:44.200 --> 01:45.280] 并不是这样的\n",
            "[01:45.280 --> 01:48.200] M4 又对微架构进行了一次调整\n",
            "[01:48.200 --> 01:51.760] 考虑到它和 M3 推出的时间仅仅相隔了半年\n",
            "[01:51.760 --> 01:54.160] 这个研发效率我觉得是有点儿东西的\n",
            "[01:54.200 --> 01:57.720] 并且这次的峰值频率也是在被动散热设备当中\n",
            "[01:57.720 --> 01:59.760] 前所未有的高\n",
            "[01:59.760 --> 02:00.400] 我们实测\n",
            "[02:00.400 --> 02:04.960] 它可以在单核负载下最高跑到 4.5GHz\n",
            "[02:04.960 --> 02:08.360] 更离谱的是小核居然也已经跑到了 2.88GHz\n",
            "[02:08.360 --> 02:12.040] 这个频率已经达到了 Intel 这代酷睿 Ultra 5 的水平\n",
            "[02:12.040 --> 02:15.760] 这颗 CPU 出现在被动散热的 iPad Pro 上\n",
            "[02:15.760 --> 02:17.240] 熟实是有点儿离谱了\n",
            "[02:17.240 --> 02:18.360] 对于 iPad 来说\n",
            "[02:18.360 --> 02:21.040] 这个频率也确实是前所未有的\n",
            "[02:21.040 --> 02:21.880] GPU 方面\n",
            "[02:21.880 --> 02:24.640] 它基本上就是 M3 GPU 的小幅优化版本\n",
            "[02:24.640 --> 02:28.160] 频率从 1.34GHz 提频到了 1.47GHz\n",
            "[02:28.160 --> 02:30.920] 也许苹果会对它做一些小优化\n",
            "[02:30.920 --> 02:34.160] 但理论上同频性能应该不会有什么明显的区别\n",
            "[02:34.160 --> 02:36.200] 另一方面是 NPU 的加强\n",
            "[02:36.200 --> 02:40.440] 这次苹果算是第一回大量在自己的发布会里提及 AI\n",
            "[02:44.440 --> 02:47.480] 更是火药味儿十足的宣称自己的 NPU 性能\n",
            "[02:47.480 --> 02:51.240] 强于目前市面上所有的 AI PC\n",
            "[02:51.240 --> 02:52.640] 这个嘲讽的是谁\n",
            "[02:52.640 --> 02:54.320] 应该不用我多说了吧\n",
            "[02:54.320 --> 02:56.760] 还有一点是发布会上没提的\n",
            "[02:56.760 --> 03:00.920] M4 已经换上了更高频率的 LPDDR5-7500 内存\n",
            "[03:00.920 --> 03:03.960] 这个应该主要是为了 AI 应用所做的准备\n",
            "[03:03.960 --> 03:07.200] 毕竟内存带宽对于 NPU 和 GPU 来说都非常重要\n",
            "[03:07.200 --> 03:08.360] 不过值得一提的是\n",
            "[03:08.360 --> 03:12.920] 它用的其实并不是我发布会后猜测的 LPDDR5X 内存\n",
            "[03:12.920 --> 03:15.680] 而是一个更高频的 LPDDR5 内存\n",
            "[03:15.680 --> 03:18.480] 这个大概率是为了延迟考虑的\n",
            "[03:18.480 --> 03:19.760] 我们也会实测一下\n",
            "[03:19.800 --> 03:22.640] 它的内存延迟表现是不是也有所提升\n",
            "[03:22.640 --> 03:25.120] 接下来我们就要聊聊 CPU 的 V 架构\n",
            "[03:25.120 --> 03:28.640] 这次依然要非常感谢俊杰1475同学的帮助\n",
            "[03:28.640 --> 03:31.360] 这次的测试过程可以说是相当的曲折\n",
            "[03:31.360 --> 03:34.520] 接下来我们就来对比一下 M2、M3、M4 三代的 V 架构\n",
            "[03:34.520 --> 03:36.920] 来看看这次到底改了什么吧\n",
            "[03:36.920 --> 03:37.880] 之前我们聊过\n",
            "[03:37.880 --> 03:41.880] A17 Pro 和 M3 可以说是苹果在 A14 和 M1 之后\n",
            "[03:41.880 --> 03:43.360] 变化最大的一代 V 架构\n",
            "[03:43.360 --> 03:46.080] 同样也是目前市面上最强力的\n",
            "[03:46.080 --> 03:49.240] 而这次 M4 的大核则是在之前优秀的架构基础上\n",
            "[03:49.240 --> 03:50.520] 再度的一次进化\n",
            "[03:50.520 --> 03:51.960] 相比于 M3 的大核\n",
            "[03:51.960 --> 03:55.000] M4 的大核在解码单元上再次加宽了\n",
            "[03:55.000 --> 03:59.560] 这次的解码单元宽度从 M3 的 9 宽再次扩张到了 10 宽\n",
            "[03:59.560 --> 04:02.680] 也就是说每个时钟周期可以解码 10 条指令\n",
            "[04:02.680 --> 04:04.480] 要知道苹果从 A14 到 A16\n",
            "[04:04.480 --> 04:07.760] 三代的架构都保持了 8 宽的解码单元设计\n",
            "[04:07.760 --> 04:11.160] 结果 A17 Pro 到 M4 短短半年多时间里面\n",
            "[04:11.160 --> 04:13.880] 直接连续两次加宽解码单元\n",
            "[04:13.880 --> 04:16.960] 架构进化的策略上这次是真的挺激进的\n",
            "[04:17.000 --> 04:20.920] 更宽的前端解码单元一般对应着更强的后端处理性能\n",
            "[04:20.920 --> 04:21.680] 我们测下来\n",
            "[04:21.680 --> 04:25.280] 后端的直行单元从数量上来看其实并没有增加\n",
            "[04:25.280 --> 04:29.560] 也许是上代的前端提升还不足以为保大幅增加的后端规模\n",
            "[04:29.560 --> 04:31.240] 这次 M4 的新架构\n",
            "[04:31.240 --> 04:35.360] 大幅增加了 Dispatch Buffer 和 5 点单元的 Scheduler 调度对列\n",
            "[04:35.360 --> 04:38.360] 这样的改进可以让内核的并行度进一步提高\n",
            "[04:38.360 --> 04:41.840] 调度对列增大也意味着更多的指令可以并行执行\n",
            "[04:41.840 --> 04:45.640] 这也就意味着处理器后端的利用效率就可以来得更高\n",
            "[04:45.640 --> 04:48.680] 再加上前面提到的前端解码部分上的改进\n",
            "[04:48.680 --> 04:52.840] 综合起来就可以让整颗处理器的指令吨度量进一步提升\n",
            "[04:52.840 --> 04:57.000] 新架构的进一步变宽也体现在 ROB TRT 深度上\n",
            "[04:57.000 --> 05:01.320] 苹果现在这颗超大核可以说是目前所有处理器架构里\n",
            "[05:01.320 --> 05:03.320] 规模最庞大的一颗\n",
            "[05:03.320 --> 05:06.080] 这也让我非常期待它的实际性能表现\n",
            "[05:06.080 --> 05:07.920] 除了更宽的微架构以外\n",
            "[05:07.920 --> 05:10.320] 这次的 M4 还引入了 SME 单元\n",
            "[05:10.320 --> 05:13.720] 你可以把它简单理解为二面板的 AVX512\n",
            "[05:13.720 --> 05:17.200] M4 的 P 核和 E 核处各有一个 SME 单元\n",
            "[05:17.200 --> 05:19.760] 它和 CPU 内核共用 L2 缓存\n",
            "[05:19.760 --> 05:21.400] 在支持 SME 的程序里\n",
            "[05:21.400 --> 05:23.400] M4 会有非常大的性能提升\n",
            "[05:23.400 --> 05:24.520] 你们也看到了\n",
            "[05:24.520 --> 05:27.080] Gigabuntz 6.3 引入了对 SME 的支持之后\n",
            "[05:27.080 --> 05:29.400] M4 的跑分有了大幅的上升\n",
            "[05:29.400 --> 05:33.440] 我们也简单测试了一下 P 核和 E 核分别的指令吨度量\n",
            "[05:33.440 --> 05:37.800] 加入 SME 的支持应该还是为了给未来的 AI 应用加速\n",
            "[05:37.800 --> 05:42.400] 年底的 ARM 公版内核也会像 M4 一样提供对 SME 2 的支持\n",
            "[05:42.440 --> 05:44.280] 我们还测了一下分支预测失误率\n",
            "[05:44.280 --> 05:45.120] 从结果来看\n",
            "[05:45.120 --> 05:47.480] M4 的分支预测能力应该没有什么改进\n",
            "[05:47.480 --> 05:48.360] 总的来说\n",
            "[05:48.360 --> 05:50.640] M4 更大的核心在 M3 的基础上\n",
            "[05:50.640 --> 05:54.000] 再次提升了前端的解码能力以及后端的并行度\n",
            "[05:54.000 --> 05:55.520] 你可以说它是进一步挖掘了\n",
            "[05:55.520 --> 05:58.360] A17 Pro 和 M3 内带微加构设计的潜力\n",
            "[05:58.360 --> 06:02.040] 它的设计目标应该就是让 A17 Pro 和 M3 里面\n",
            "[06:02.040 --> 06:03.760] 增加的后端执行单元\n",
            "[06:03.760 --> 06:05.880] 尽可能去火力全开\n",
            "[06:05.880 --> 06:08.400] 我们还测试了 M4 的内存与缓存性能\n",
            "[06:08.400 --> 06:12.200] 可以看到 M4 的内存延迟明显比 M3 有所降低\n",
            "[06:12.200 --> 06:15.160] 延迟控制已经来到了 88ns 的水平\n",
            "[06:15.160 --> 06:17.960] 比起上代 M3 的 96ns 左右有所改善\n",
            "[06:17.960 --> 06:23.680] 这个应该是用 LPDDR5 7500 而非 LPDDR5 X 的主要原因了\n",
            "[06:23.680 --> 06:24.760] 至于小核心\n",
            "[06:24.760 --> 06:28.720] M4 的小核心依然沿用了 A17 Pro 和 M3 小核的微加构\n",
            "[06:28.720 --> 06:30.600] 只是频率更高而已\n",
            "[06:30.600 --> 06:33.360] 这个就是 M4 大小核完整的架构图\n",
            "[06:33.360 --> 06:36.200] 感兴趣的同学也可以截图自己来研究研究\n",
            "[06:36.240 --> 06:37.440] 了解了架构之后\n",
            "[06:37.440 --> 06:40.320] 我们就该请出业界公认的跨平台测试标准\n",
            "[06:40.320 --> 06:41.960] SPEC 2017 了\n",
            "[06:41.960 --> 06:44.720] 来直观的看看性能提升幅度\n",
            "[06:44.720 --> 06:46.440] 这次的 SPEC 2017 当中\n",
            "[06:46.440 --> 06:49.960] 我们依旧测出了精确的内核功耗和实时的核心频率\n",
            "[06:49.960 --> 06:51.760] M4 放在 iPad 里\n",
            "[06:51.760 --> 06:54.520] 不可避免的要受到被动散热降频的影响\n",
            "[06:54.520 --> 06:55.920] 但 M4 本质上\n",
            "[06:55.920 --> 06:57.720] 却并不是一颗移动 SoC\n",
            "[06:57.720 --> 07:01.360] 它的封装方式、内存结构、频率策略等等\n",
            "[07:01.360 --> 07:03.840] 都和手机处理器有比较大的差别\n",
            "[07:03.840 --> 07:06.120] 显然 4.5GHz 的风执频率\n",
            "[07:06.120 --> 07:08.520] 根本就不是为了 iPad 准备的\n",
            "[07:08.520 --> 07:11.160] 所以首先为了能够测出它的风执表现\n",
            "[07:11.160 --> 07:12.920] 我们会通过液氮来制冷\n",
            "[07:12.920 --> 07:15.360] 把芯片表面的温度降低到零度以下\n",
            "[07:15.360 --> 07:16.240] 来推测一下\n",
            "[07:16.240 --> 07:19.400] 它在主动散热的 Mac 平台会有什么样的表现\n",
            "[07:19.400 --> 07:23.120] 也能让我们前瞻到 M4 Pro 和 M4 Max\n",
            "[07:23.120 --> 07:25.800] 显然我们频道相比于 iPad 本身\n",
            "[07:25.800 --> 07:29.880] 更关心的是里面这颗 M4 芯片本身的技术水平\n",
            "[07:29.880 --> 07:31.360] 为了看得更全面一点\n",
            "[07:31.360 --> 07:33.240] 极限风执肯定是要测的\n",
            "[07:33.280 --> 07:36.400] 但是我们也会展现一下 M4 在 iPad 下的表现\n",
            "[07:36.400 --> 07:39.960] 以及和 M3 拉来对比一下能效\n",
            "[07:39.960 --> 07:42.080] 包括这次我们还引入了一个新的测试\n",
            "[07:42.080 --> 07:46.000] 我们可以利用 Xcode 模拟它在轻度发热降频时的频率表现\n",
            "[07:46.000 --> 07:48.120] 这个时候频率会接近 4GHz\n",
            "[07:48.120 --> 07:50.640] 这样一来我们就可以非常公平的\n",
            "[07:50.640 --> 07:53.600] 和 M3 横向对比同频性的功耗了\n",
            "[07:53.600 --> 07:55.720] 接下来我们直接来看性能\n",
            "[07:55.720 --> 07:57.440] 新架构加上高频率\n",
            "[07:57.440 --> 07:59.200] 性能自然是差不了的\n",
            "[07:59.200 --> 08:00.480] 从绝对性能上来说\n",
            "[08:00.480 --> 08:02.720] M4 这颗大核的峰值性能\n",
            "[08:02.720 --> 08:06.960] 几乎比 M3 的大核高出了 20%\n",
            "[08:06.960 --> 08:10.840] 半年时间提升 20% 的绝对性能\n",
            "[08:10.840 --> 08:13.080] 这颗 CPU 应该是 M1 推出之后\n",
            "[08:13.080 --> 08:14.200] 到今天为止\n",
            "[08:14.200 --> 08:16.480] 单核性能提升最大的一次\n",
            "[08:16.480 --> 08:19.880] 但超强的性能也带来了明显拔高的功耗\n",
            "[08:19.880 --> 08:21.600] 两者都跑峰值性能的情况下\n",
            "[08:21.600 --> 08:23.280] M4 的大核仅仅是内核功耗\n",
            "[08:23.280 --> 08:25.760] 就已经比 M3 高出了 60%\n",
            "[08:25.760 --> 08:29.320] 用 60% 的峰值功耗换 20% 的峰值性能\n",
            "[08:29.360 --> 08:31.560] 这个操作好像有点英克尔\n",
            "[08:31.560 --> 08:34.480] 或者说它有点像 AMD PBO 的思路\n",
            "[08:34.480 --> 08:36.600] 大幅拉高功耗预算\n",
            "[08:36.600 --> 08:39.280] 去拉高处理器的极限单核性能\n",
            "[08:39.280 --> 08:43.200] 这个就跟苹果 M1 和 M2 的逻辑完全不一样\n",
            "[08:43.200 --> 08:46.000] 这种玩法对于 PC 来说是合理的\n",
            "[08:46.000 --> 08:48.120] 但是它显然并不适合手机\n",
            "[08:48.120 --> 08:51.120] 所以你要拿 M4 去和手机斗曲却\n",
            "[08:51.120 --> 08:52.800] 去比这个单核功耗\n",
            "[08:52.800 --> 08:55.040] 可能很难斗出个所以然\n",
            "[08:55.040 --> 08:56.720] 因为这个东西的竞争对手\n",
            "[08:56.720 --> 08:59.000] 其实是酷睿 Ultra 和 Ryzen\n",
            "[08:59.000 --> 09:02.080] 比起找一个这种非常甜点的单核频率\n",
            "[09:02.080 --> 09:04.840] PC 其实更需要在功耗预算里面\n",
            "[09:04.840 --> 09:07.760] 把极限单线程的性能拉高\n",
            "[09:07.760 --> 09:08.680] 说实话\n",
            "[09:08.680 --> 09:12.280] 这似乎是有背于苹果前两年的哲学的\n",
            "[09:12.280 --> 09:13.240] 你们还记不记得\n",
            "[09:13.240 --> 09:16.200] 22 年的时候我们采访 Apple Silicon 的大佬\n",
            "[09:16.200 --> 09:17.880] 问他们为什么不超频\n",
            "[09:17.880 --> 09:19.200] 为什么不超频\n",
            "[09:19.200 --> 09:20.800] 当时他们可会说话了\n",
            "[09:20.800 --> 09:22.000] 言下之意就是\n",
            "[09:22.000 --> 09:24.720] 我们跟英克尔和 AMD 可不一样\n",
            "[09:24.720 --> 09:26.760] 反正我们不超频\n",
            "[09:26.760 --> 09:28.440] 能效第一位\n",
            "[09:28.480 --> 09:29.560] 结果现在\n",
            "[09:29.560 --> 09:30.560] 真香了吧\n",
            "[09:30.560 --> 09:32.160] 考虑到 Mac 上的性能需求\n",
            "[09:32.160 --> 09:34.960] 能把极限性能拉上去肯定是好的\n",
            "[09:34.960 --> 09:37.680] 只要不要影响中低频的能效就行了\n",
            "[09:37.680 --> 09:38.520] 当然\n",
            "[09:38.520 --> 09:41.160] 这颗 M4 现在是搭载在 iPad 上的\n",
            "[09:41.160 --> 09:43.640] 这个极限性能其实并不好测出来\n",
            "[09:43.640 --> 09:44.280] 事实上\n",
            "[09:44.280 --> 09:46.240] 如果你使用常温来测试的话\n",
            "[09:46.240 --> 09:48.400] 它的频率策略会保守很多\n",
            "[09:48.400 --> 09:50.200] 我们实测 M4 的锐频策略\n",
            "[09:50.200 --> 09:53.080] 各种意义上都很像英克尔和 AMD 的策略\n",
            "[09:53.080 --> 09:54.680] 在低温单线程下\n",
            "[09:54.680 --> 09:57.040] 最高频率可以狂飙到 4.5GHz\n",
            "[09:57.080 --> 09:59.440] 而常温下或者是双线程的时候\n",
            "[09:59.440 --> 10:01.040] 则是跑在 4.4GHz\n",
            "[10:01.040 --> 10:04.640] 三线程以上则会降低到全核 3.94GHz\n",
            "[10:04.640 --> 10:06.320] 也就是 4GHz 附近的频率\n",
            "[10:06.320 --> 10:07.880] 和其他 PC 处理器一样\n",
            "[10:07.880 --> 10:11.360] 每个大核都可以跑到最高的单核锐频\n",
            "[10:11.360 --> 10:13.200] 而并不是像手机 CPU 那样\n",
            "[10:13.200 --> 10:15.360] 只有一个核心去跑相对高频\n",
            "[10:15.360 --> 10:18.000] 而其他核心只能跑相对低频的状态\n",
            "[10:18.000 --> 10:20.440] 如果我们考虑常温状态下的性能和功耗\n",
            "[10:20.440 --> 10:22.200] M4 的表现又如何呢?\n",
            "[10:22.200 --> 10:23.880] 刚好我们可以通过 Xcode\n",
            "[10:23.880 --> 10:26.280] 给 iPad Pro 模拟一个轻度降频的功耗\n",
            "[10:26.280 --> 10:27.560] 在这种情况下\n",
            "[10:27.560 --> 10:30.200] 它的频率就会跑到和 M3 近似的水平\n",
            "[10:30.200 --> 10:33.120] 基本就是跑在平均 4GHz 附近的频点下\n",
            "[10:33.120 --> 10:34.240] 这个时候\n",
            "[10:34.240 --> 10:38.400] M4 的性能大致会比 M3 高 8.5% 左右\n",
            "[10:38.400 --> 10:41.320] 功耗则高出 M3 5% 左右\n",
            "[10:41.320 --> 10:42.800] 那么从这个成绩来看\n",
            "[10:42.800 --> 10:46.440] M4 大核的能耗比只有非常小的进步\n",
            "[10:46.440 --> 10:48.560] 可能是架构带来的性能提升\n",
            "[10:48.560 --> 10:50.720] 抵消了新工艺的能效优势\n",
            "[10:50.720 --> 10:53.840] 也可能是新工艺主要提升了跑高频能力\n",
            "[10:53.880 --> 10:56.320] 但并没有明显改善的能效\n",
            "[10:56.320 --> 11:00.040] 这个就要等到我们待会测试 GPU 功耗的时候\n",
            "[11:00.040 --> 11:01.320] 再去做验证了\n",
            "[11:01.320 --> 11:04.680] 然后我们就来对比一下新架构的同频性能\n",
            "[11:04.680 --> 11:09.160] M4 的大核 IPC 算下来要比 M3 整数高出 7.3% 左右\n",
            "[11:09.160 --> 11:11.080] 幅点高出 8.6% 左右\n",
            "[11:11.080 --> 11:11.920] 平均下来\n",
            "[11:11.920 --> 11:14.480] 这次的 IPC 提升接近 8%\n",
            "[11:14.480 --> 11:17.080] 虽然还是没有两位数的提升\n",
            "[11:17.080 --> 11:19.320] 但这已经是自 A14 推出以来\n",
            "[11:19.320 --> 11:21.840] 苹果 IPC 提升幅度最大的一次了\n",
            "[11:21.840 --> 11:23.000] 它也再次巩固了\n",
            "[11:23.000 --> 11:25.600] 苹果在处理器架构上的领先地位\n",
            "[11:25.600 --> 11:27.040] 然而在大核能效方面\n",
            "[11:27.040 --> 11:30.440] 用上 N3E 的 M4 却并没有给我们带来惊喜\n",
            "[11:30.440 --> 11:33.560] 这 N3E 的能效真的进步了吗?\n",
            "[11:33.560 --> 11:37.400] 看来年底的 A18 Pro 依然是悬念很大\n",
            "[11:37.400 --> 11:42.200] M4 的新大核如果和目前 X86 阵营最强的 14900K\n",
            "[11:42.200 --> 11:43.920] 去对比峰值单核性能\n",
            "[11:43.920 --> 11:45.560] 它的水平又如何呢?\n",
            "[11:45.560 --> 11:46.080] 诶\n",
            "[11:46.080 --> 11:49.600] M4 在 4.5GHz 下比 6GHz 的 14900K\n",
            "[11:49.600 --> 11:50.720] SPAC 2017 里面\n",
            "[11:50.720 --> 11:53.120] Int 成绩高出了大约 18%\n",
            "[11:53.120 --> 11:56.080] FB 成绩更是高出了 26%\n",
            "[11:56.080 --> 11:56.880] 综合来看\n",
            "[11:56.880 --> 12:01.920] M4 的单核性能已经比 14900K 高出了 23%\n",
            "[12:01.920 --> 12:07.040] 如果说 M3 的任务是单核性能和 14900K 去打个平手\n",
            "[12:07.040 --> 12:11.800] 这次 M4 的任务就是彻底干碎 14900K\n",
            "[12:11.800 --> 12:12.840] 我真的很好奇\n",
            "[12:12.840 --> 12:15.280] 英克尔今年到底要做什么\n",
            "[12:15.280 --> 12:18.320] 才能原地提升 23% 的单核性能\n",
            "[12:18.360 --> 12:21.800] 来重新夺回这个最强单核性能的宝座\n",
            "[12:21.800 --> 12:23.760] 当年在测试 M1 的时候我就说\n",
            "[12:23.760 --> 12:25.920] 苹果的架构是很强\n",
            "[12:25.920 --> 12:27.280] 可是跑不了高频\n",
            "[12:27.280 --> 12:31.160] 所以单核性能其实依然会被当年的 12900K 压着打\n",
            "[12:31.160 --> 12:32.320] 然而时过境迁\n",
            "[12:32.320 --> 12:33.240] IPC 更高\n",
            "[12:33.240 --> 12:34.920] 还跑上了高频的 M4\n",
            "[12:34.920 --> 12:37.840] 面对的对手却是 IPC 原地踏步\n",
            "[12:37.840 --> 12:40.880] 只有不断超频维持颜面的 14900K\n",
            "[12:40.880 --> 12:43.680] 甚至英克尔最近还因为超频超过头\n",
            "[12:43.680 --> 12:46.480] 而在最新的固件里被迫降频\n",
            "[12:46.480 --> 12:47.080] 说真的\n",
            "[12:47.080 --> 12:49.120] 我现在反而更期待 AMD Zen5 了\n",
            "[12:49.120 --> 12:52.200] 简直就是 X86 全村的希望\n",
            "[12:52.200 --> 12:54.240] 加油啊 AMD\n",
            "[12:54.240 --> 12:56.040] 那么聊完了性能超强的大核\n",
            "[12:56.040 --> 12:58.160] 我们也来看看小核的成绩\n",
            "[12:58.160 --> 13:00.560] 由于小核架构并没有任何的变化\n",
            "[13:00.560 --> 13:03.200] 所以它的性能仅仅是来自于频率的提升\n",
            "[13:03.200 --> 13:06.520] 这个频率的小核也不大可能出现在手机端\n",
            "[13:06.520 --> 13:07.640] 我大胆猜测\n",
            "[13:07.640 --> 13:09.200] 这次 M4 用上 6 小核\n",
            "[13:09.200 --> 13:14.240] 可能是意味着苹果从 M3 Pro 开始引入的 6 小核为一组核心素\n",
            "[13:14.280 --> 13:17.360] 可能会成为未来苹果处理器设计的主流\n",
            "[13:17.360 --> 13:19.640] 这是不是会意味着我们未来能够看到\n",
            "[13:19.640 --> 13:22.360] 2加6核心设计的 A 系列处理器呢\n",
            "[13:22.360 --> 13:23.680] 我们拭目以待\n",
            "[13:23.680 --> 13:25.600] 总结一下 SPAC 2017 的成绩\n",
            "[13:25.600 --> 13:27.800] M4 这个新架构高频率的大核\n",
            "[13:27.800 --> 13:29.520] 跑出了非常离谱的性能\n",
            "[13:29.520 --> 13:33.680] 峰值性能比 14900K 高出 23%\n",
            "[13:33.680 --> 13:37.160] 让人非常好奇如果 M4 摆脱了 iPad 的身躯\n",
            "[13:37.160 --> 13:40.160] 来到 Mac 平台上又会爆发出什么样的性能\n",
            "[13:40.160 --> 13:42.320] 那么在 M3 推出仅仅半年之后\n",
            "[13:42.320 --> 13:46.240] 再次提升 8.6% 的 IPC 也真的是挺出乎意料的\n",
            "[13:46.240 --> 13:48.800] 这也更让人期待未来的 M4 Pro\n",
            "[13:48.800 --> 13:52.680] M4 Max 甚至于 M4 Ultra 会有多强了\n",
            "[13:52.680 --> 13:55.360] 显然 Mac 才是 M4 家族真正的舞台\n",
            "[13:55.360 --> 13:59.680] iPad Pro 上的 M4 也只能算得上是一道开胃菜\n",
            "[13:59.680 --> 14:01.200] 然而我对这颗新核心\n",
            "[14:01.200 --> 14:04.520] 在未来手机的 A 系列处理器上的表现\n",
            "[14:04.520 --> 14:05.920] 其实并没有什么信心\n",
            "[14:05.920 --> 14:07.440] 因为用上 N3E 的它\n",
            "[14:07.440 --> 14:11.920] 同频能耗比仅仅比 M3 的大核强了 3% 左右\n",
            "[14:11.960 --> 14:13.800] 那这个 N3E 的能耗比提升\n",
            "[14:13.800 --> 14:16.280] 看起来也没有想象中那么大\n",
            "[14:16.280 --> 14:19.880] 这个人类科技难道真的是被质子锁死了吗\n",
            "[14:19.880 --> 14:22.200] 更大的核心一定会带来更高的功耗\n",
            "[14:22.200 --> 14:23.560] 所以对手机来说\n",
            "[14:23.560 --> 14:26.880] 可能多加点小核去提升多核的能耗比\n",
            "[14:26.880 --> 14:28.280] 才是真正的出入\n",
            "[14:28.280 --> 14:32.320] 那么天玑 9000 III 和骁龙 8000 III 就已经证明了这一点\n",
            "[14:32.320 --> 14:33.720] 那么其实测 M 系列处理器\n",
            "[14:33.720 --> 14:35.840] 我是一直很抗拒用 Geekbench\n",
            "[14:35.840 --> 14:38.160] 毕竟单核咱们都已经测了 Specs 了\n",
            "[14:38.160 --> 14:40.760] 多核我如果在 Mac 上直接跑个 3Dbench\n",
            "[14:40.760 --> 14:43.240] 不就可以轻松对比性能和功耗了吗\n",
            "[14:43.240 --> 14:46.400] 而且很方便去对比 Intel 和 AMD 的 CPU\n",
            "[14:46.400 --> 14:50.560] 然而 苹果这次居然先把 M4 塞进了 iPad 里了\n",
            "[14:50.560 --> 14:52.920] 好吧 刚好咱们也来跑个 Geekbench\n",
            "[14:52.920 --> 14:57.520] 来看看 M4 如果加入移动处理器的赛博斗躯躯套餐\n",
            "[14:57.520 --> 15:03.600] 来看看 PC 处理器和手机处理器的性能和功耗到底相差有多大\n",
            "[15:03.600 --> 15:05.960] 这次我们不仅测试了满血 M4\n",
            "[15:05.960 --> 15:08.680] 也测试了低配版里面搭载的 3 加 6 核\n",
            "[15:08.680 --> 15:10.360] 三缸清纯版 M4\n",
            "[15:10.400 --> 15:14.040] 我一定要看看苹果这一刀到底砍掉了多少性能\n",
            "[15:14.040 --> 15:16.320] 以及它们的能效差距到底大不大\n",
            "[15:16.320 --> 15:19.880] 首先我们来看看强制冷却下 Geekbench 6 的峰值性能\n",
            "[15:19.880 --> 15:23.160] M4 的单核跑出了接近 4000 分的离谱成绩\n",
            "[15:23.160 --> 15:27.480] 这个成绩似乎比起 M3 直接高出了 26%\n",
            "[15:27.480 --> 15:33.320] 也比至今仍未上市的高通 XEL的功课的官方成绩高出了 1000 分左右\n",
            "[15:33.320 --> 15:38.000] 但这么高的成绩其实也是因为 Geekbench 6.3 加入了 SME 的支持\n",
            "[15:38.040 --> 15:41.360] 所以 M4 在新版 Geekbench 6 里面会有明显的优势\n",
            "[15:41.360 --> 15:44.200] 我觉得 Geekbench 6 这个成绩只能说仅供参考吧\n",
            "[15:44.200 --> 15:48.360] 可能还是前面咱们测的 SPAC 2017 的成绩更具有普遍性于\n",
            "[15:48.360 --> 15:52.200] 多核方面满血的 M4 跑到了接近 15000 分的成绩\n",
            "[15:52.200 --> 15:54.680] 三加六核的残血版 M4 则要低一些\n",
            "[15:54.680 --> 15:56.400] 大概 13500 出头\n",
            "[15:56.400 --> 16:00.160] 但即便是残血版 M4 也依然比 M3 的成绩要高\n",
            "[16:00.160 --> 16:02.960] 满血版甚至可以直逼 M3 Pro\n",
            "[16:02.960 --> 16:06.280] 而在还没有针对 SME 优化的 Geekbench 5 当中\n",
            "[16:06.320 --> 16:08.200] M4 的表现依然很强\n",
            "[16:08.200 --> 16:12.720] 2750 分左右的单核成绩比起 M3 大约提升了 17% 左右\n",
            "[16:12.720 --> 16:16.400] 这个提升幅度其实也很接近前面 SPAC 2017 的提升幅度\n",
            "[16:16.400 --> 16:17.400] 多核方面\n",
            "[16:17.400 --> 16:19.640] 满血版 M4 凭借着更高的规格\n",
            "[16:19.640 --> 16:21.520] 更高的频率以及更高的 FPC\n",
            "[16:21.520 --> 16:24.200] 轻松超过 M3 25%\n",
            "[16:24.200 --> 16:27.440] 即便是残血版也能够超过 M3 10%\n",
            "[16:27.440 --> 16:30.600] 如果你要进一步对比上代 iPad Pro 的 M2 的话\n",
            "[16:30.600 --> 16:32.880] 不管是单核还是多核的差距\n",
            "[16:32.880 --> 16:35.560] 都已经拉大到了接近 50%\n",
            "[16:35.560 --> 16:37.360] 这提升幅度确实有点大\n",
            "[16:37.360 --> 16:38.880] 如果拿 Geekbench 5 的成绩\n",
            "[16:38.880 --> 16:41.800] 和 Windows 阵营的主流笔记本处理器对比的话\n",
            "[16:41.800 --> 16:43.440] M4 的多核成绩比较接近\n",
            "[16:43.440 --> 16:47.960] 功耗跑满的酷睿 Ultra 9 185H 和 i9 13900H\n",
            "[16:47.960 --> 16:51.960] 比起 R9 8945HS 要强一点\n",
            "[16:51.960 --> 16:53.760] 它的单核成绩就更不用说了\n",
            "[16:53.760 --> 16:57.000] 目前没有任何对手可以接近\n",
            "[16:57.000 --> 17:00.520] 当然用 Geekbench 去跑 PC 芯片的性能\n",
            "[17:00.520 --> 17:01.680] 它是有局限性的\n",
            "[17:01.680 --> 17:03.720] 那么等到 M4 真正上了 Mac 之后\n",
            "[17:03.720 --> 17:06.080] 我们也会再来跑一个 Cinebench 2024\n",
            "[17:06.080 --> 17:10.480] 来看看它更加接近哪款 Windows 的笔记本处理器\n",
            "[17:10.480 --> 17:13.680] M4 的峰值性能看下来真的是非常强\n",
            "[17:13.680 --> 17:15.640] 它的功耗呢\n",
            "[17:15.640 --> 17:16.920] 在峰值状态下\n",
            "[17:16.920 --> 17:20.800] M4 满血版在 Geekbench 5 里跑到了 30W 的功耗\n",
            "[17:20.800 --> 17:23.440] 这个功耗比起 M3 和 M2 来的要更高一点\n",
            "[17:23.440 --> 17:25.680] 残血版则是跑出了 25W 的功耗\n",
            "[17:25.680 --> 17:28.560] 这个功耗就比较接近 M3 和 M2 的水平了\n",
            "[17:28.560 --> 17:30.040] 这个功耗虽然高\n",
            "[17:30.040 --> 17:31.280] 但是我们早就说过\n",
            "[17:31.280 --> 17:32.640] 我们不光要看极限\n",
            "[17:32.680 --> 17:34.360] 也要再跑一下省电模式\n",
            "[17:34.360 --> 17:36.880] 才能判断出大概的能效表现\n",
            "[17:36.880 --> 17:37.760] 在省电模式下\n",
            "[17:37.760 --> 17:40.880] 满血版 M4 的成绩和功耗都比 M3 高一些\n",
            "[17:40.880 --> 17:43.720] 但是残血版 M4 的成绩就要比 M3 低一点了\n",
            "[17:43.720 --> 17:45.560] 但是功耗又要高一点\n",
            "[17:45.560 --> 17:48.480] 那 M2 的省电模式的频点反而还比较激进\n",
            "[17:48.480 --> 17:50.360] 总体能效大概是这样的\n",
            "[17:50.360 --> 17:52.400] M4 的能效进步是有的\n",
            "[17:52.400 --> 17:55.120] 但是还是得感谢加了这两个小核\n",
            "[17:55.120 --> 17:57.240] 如果不是因为这堆规模的话呢\n",
            "[17:57.240 --> 18:00.280] 多核能效可能也真的并不会有什么进步\n",
            "[18:00.320 --> 18:02.840] 三缸版的 M4 就已经能够说明这个问题了\n",
            "[18:02.840 --> 18:07.080] 当然我们也可以加入目前的旗舰手机能效来对比看看\n",
            "[18:08.480 --> 18:11.880] 这个曲线的走向真是跟手机完全不一样\n",
            "[18:11.880 --> 18:15.520] M4 这个能效和手机的表现差别确实很大\n",
            "[18:15.520 --> 18:18.280] 高性能段 M 系列毫无疑问会掉达手机\n",
            "[18:18.280 --> 18:19.560] 但到了省电模式\n",
            "[18:19.560 --> 18:21.000] 能效就会接近手机\n",
            "[18:21.000 --> 18:23.560] 你可以看到 3 加 6 的 M4 在省电模式下\n",
            "[18:23.560 --> 18:25.880] 已经快要贴到 8 正 3 的曲线了\n",
            "[18:25.880 --> 18:27.080] 如果再往低频走\n",
            "[18:27.080 --> 18:29.400] 能效可能就会被手机芯片反超\n",
            "[18:29.400 --> 18:32.040] 毕竟 M 系列的基础功耗是高于手机的\n",
            "[18:32.040 --> 18:32.960] 但有一说一\n",
            "[18:32.960 --> 18:34.720] 更多的跑在低频下\n",
            "[18:34.720 --> 18:36.320] 且高 IPC 的核心\n",
            "[18:36.320 --> 18:39.680] 确实是目前提升多核能效的不二法门\n",
            "[18:39.680 --> 18:41.960] 那么总结一下 M4 CPU 部分的表现\n",
            "[18:41.960 --> 18:43.800] 30W 的峰值主板功耗\n",
            "[18:43.800 --> 18:45.720] 换来了和 Intel Ultra 9\n",
            "[18:45.720 --> 18:48.520] AMD Ryzen 9 笔记本同级的多核性能\n",
            "[18:48.520 --> 18:49.520] 换句话说就是\n",
            "[18:49.520 --> 18:52.240] 目前 iPad Pro 的 CPU 峰值性能\n",
            "[18:52.240 --> 18:54.680] 甚至是和 ROG 换插接近的\n",
            "[18:54.680 --> 18:58.000] 虽然我并不知道 iPad Pro 拥有这样的性能可以用来做什么\n",
            "[18:58.000 --> 18:59.040] 但我非常好奇\n",
            "[18:59.040 --> 19:00.400] 在 MacBook Pro 上\n",
            "[19:00.400 --> 19:03.320] M4 这套新架构如果对足够高的规模\n",
            "[19:03.320 --> 19:07.240] 比如 M4 Max 去做一个 12 大核 6 小核的规格\n",
            "[19:07.240 --> 19:09.440] 那它的性能得有多强啊\n",
            "[19:09.440 --> 19:11.160] 那这个时候 X8U 阵营\n",
            "[19:11.160 --> 19:14.840] 是不是还真的能够拿出足以应付它的笔记本处理器\n",
            "[19:14.840 --> 19:15.600] 但另一方面\n",
            "[19:15.600 --> 19:18.760] 能效才是移动处理器最应该在乎的部分\n",
            "[19:18.760 --> 19:20.240] 就目前的测试来看\n",
            "[19:20.240 --> 19:24.040] M4 的能效基本上就来自于那两个新加的小核\n",
            "[19:24.040 --> 19:24.960] 架构也好\n",
            "[19:24.960 --> 19:25.840] 工艺也好\n",
            "[19:25.840 --> 19:27.960] 似乎都并没有带来更好的能效\n",
            "[19:28.000 --> 19:29.320] 再来聊聊 GPU 吧\n",
            "[19:29.320 --> 19:30.120] 前面我也说了\n",
            "[19:30.120 --> 19:32.520] M4 的 GPU 并没有带来什么新鲜的东西\n",
            "[19:32.520 --> 19:35.200] 10% 的频率也带来了 10% 的性能\n",
            "[19:35.200 --> 19:36.840] 并没有什么值得聊的地方\n",
            "[19:36.840 --> 19:39.840] 如果你想更多了解这颗 GPU 的细节和性能\n",
            "[19:39.840 --> 19:42.200] 可以去看一下我们对 M3 的详细评测\n",
            "[19:42.200 --> 19:45.280] 当时我已经对这颗 GPU 做了足够多的测试\n",
            "[19:45.280 --> 19:46.360] 比起性能\n",
            "[19:46.360 --> 19:49.720] GPU 维持相同的架构可以让我们更直观地看到\n",
            "[19:49.720 --> 19:53.600] N3E 工艺能耗比是不是会比上代进步一点\n",
            "[19:53.600 --> 19:54.440] 事实上\n",
            "[19:54.480 --> 19:58.520] M4 GPU 的峰值功耗比起 M3 的确有了显著的拉高\n",
            "[19:58.520 --> 19:59.360] 和 CPU 类似\n",
            "[19:59.360 --> 20:00.840] 也是从 25W 左右\n",
            "[20:00.840 --> 20:02.320] 涨到了 30W 左右\n",
            "[20:02.320 --> 20:06.720] 如果我们把 M4 控制在和 M3 接近的 8300 分左右时\n",
            "[20:06.720 --> 20:09.560] 它们的功耗也就基本维持了同一个水平\n",
            "[20:09.560 --> 20:12.760] 看来 N3E 的高频能效并没有任何惊喜\n",
            "[20:12.760 --> 20:14.120] 那低频能效呢\n",
            "[20:14.120 --> 20:16.720] 两颗 GPU 我们都跑在省电模式\n",
            "[20:16.720 --> 20:19.440] M4 GPU 在成绩稍高于 M3 的同时\n",
            "[20:19.440 --> 20:20.960] 功耗也要稍低一点\n",
            "[20:20.960 --> 20:23.200] 但这两个点其实也很接近\n",
            "[20:23.200 --> 20:26.440] 可以说低频下的确有一点点能效提升\n",
            "[20:26.440 --> 20:29.280] 但真的也就只有一点点而已\n",
            "[20:29.280 --> 20:30.080] 所以测到现在\n",
            "[20:30.080 --> 20:31.200] 答案就很明确了\n",
            "[20:31.200 --> 20:33.000] N3E 相比 N3B 工艺\n",
            "[20:33.000 --> 20:35.520] 并没有带来什么能耗比上的显示进步\n",
            "[20:35.520 --> 20:41.280] 抽象的是上一代的 N3B 其实也没有比上上代的 N4P 能耗比进步多少\n",
            "[20:41.280 --> 20:44.400] 其实这种现状对苹果来说还真是蛮亏的\n",
            "[20:44.400 --> 20:46.800] 花大价钱用最先进的制程\n",
            "[20:46.800 --> 20:49.720] 却并不能像以前那样获得能效的 buff\n",
            "[20:49.760 --> 20:51.640] 如果你期待 N3E 工艺的进步\n",
            "[20:51.640 --> 20:54.880] 能为年底的手机处理器带来什么新气象的话\n",
            "[20:54.880 --> 20:56.200] 那你可能要失望了\n",
            "[20:56.200 --> 21:00.120] 还是期待一下各家在架构上的改进更现实一点\n",
            "[21:00.120 --> 21:02.440] 当然这次我也测试了一个很有趣的东西\n",
            "[21:02.440 --> 21:05.680] 如果把 M4 GPU 降频到手机的性能级别\n",
            "[21:05.680 --> 21:07.200] 它的功耗会怎么样呢\n",
            "[21:07.200 --> 21:09.480] 在和 iPhone 接近 4400 分附近\n",
            "[21:09.480 --> 21:13.280] M4 GPU 跑 WLE 的功耗只有 6W 出头\n",
            "[21:13.280 --> 21:16.320] 显然这功耗可比 A17 Pro 低太多了\n",
            "[21:16.320 --> 21:19.400] 这种状态下的能效干翻所有手机芯片\n",
            "[21:19.400 --> 21:20.800] 是非常轻松的\n",
            "[21:20.800 --> 21:24.520] 看来 A17 Pro 的 GPU 规模明显太小了\n",
            "[21:24.520 --> 21:27.760] 那能耗表现怎么能干得过人家高通和发哥呢\n",
            "[21:27.760 --> 21:30.400] 人家可是实打实的在堆规格\n",
            "[21:30.400 --> 21:35.480] 所以苹果能不能给 A 系列处理器搞一个规模狠一点的 GPU 呢\n",
            "[21:35.480 --> 21:36.400] 在移动端\n",
            "[21:36.400 --> 21:41.080] 一颗跑低频的大规模 GPU 才是好闻名啊\n",
            "[21:41.080 --> 21:41.800] 测到现在\n",
            "[21:41.800 --> 21:44.160] 你应该已经基本了解了 M4 的性能表现\n",
            "[21:44.160 --> 21:45.440] 但是到目前为止\n",
            "[21:45.440 --> 21:47.600] 我们都还没有开始聊 iPad 呢\n",
            "[21:47.640 --> 21:48.360] 所以接下来\n",
            "[21:48.360 --> 21:51.440] 我们测试的焦点要放回到 iPad Pro 了\n",
            "[21:51.440 --> 21:53.080] 那么首先先从游戏开始\n",
            "[21:53.080 --> 21:55.080] 咱们首先来看看原神\n",
            "[21:55.080 --> 21:59.200] 原神这次非常少见的在新 iPad 发售当天就进行了适配\n",
            "[21:59.200 --> 22:00.680] 我们实际数了分辨率\n",
            "[22:00.680 --> 22:04.000] 13寸的分辨率高达 2292 x 1718\n",
            "[22:04.000 --> 22:06.760] 11寸则是在 2292 x 1600 左右\n",
            "[22:06.760 --> 22:10.000] 这个渲染压力和 2560 x 1440 基本一致\n",
            "[22:10.000 --> 22:12.400] 那么 13寸版本则会稍高一点\n",
            "[22:12.400 --> 22:13.840] 在这么高的分辨率下\n",
            "[22:13.840 --> 22:15.120] 在虚弥层跑图\n",
            "[22:15.120 --> 22:17.600] M4 的 iPad Pro 它能撑得住吗?\n",
            "[22:17.600 --> 22:18.480] 事实证明啊\n",
            "[22:18.480 --> 22:22.720] 这点压力对于 M4 iPad Pro 来说实在是过于轻松了\n",
            "[22:22.720 --> 22:26.000] 不管是三台新 iPad Pro 当中的哪一台\n",
            "[22:26.000 --> 22:28.880] 都跑出了 60 帧一根直线的平均帧\n",
            "[22:28.880 --> 22:29.760] 结合功耗来看\n",
            "[22:29.760 --> 22:34.120] 这个游戏能效表现明显比起 M2 iPad 强了很多\n",
            "[22:34.120 --> 22:38.040] 并且最关键的是这个低配版 3 缸 M4 和 4 缸 M4\n",
            "[22:38.040 --> 22:42.000] 其实在实际游戏的帧数和功耗上面都没有任何区别\n",
            "[22:42.000 --> 22:42.600] 相比之下\n",
            "[22:42.640 --> 22:45.880] M2 在测试的后半程则基本只能跑在 50 帧左右\n",
            "[22:45.880 --> 22:49.200] 难道这个米哈游说原神 5.0 要提升性能压力\n",
            "[22:49.200 --> 22:51.720] 是因为他们已经提前玩到 M4 了吗?\n",
            "[22:51.720 --> 22:58.400] 另外 M2 的 13 寸 Air 比起上代使用 MiniLED 屏幕的 12.9 寸 iPad Pro 也要好不少\n",
            "[22:58.400 --> 23:01.200] 帧数更高的同时平均功耗更低不少\n",
            "[23:01.200 --> 23:04.840] 看起来应该是上代的 MiniLED 实在是太费电了\n",
            "[23:04.840 --> 23:08.560] 包括这代的 M4 iPad 在游戏功耗上的优势\n",
            "[23:08.560 --> 23:11.520] 可能有很大一部分是这个 OLED 屏幕所带来的\n",
            "[23:11.560 --> 23:13.120] 发热方面也是充满惊喜\n",
            "[23:13.120 --> 23:13.920] 测试结束之后\n",
            "[23:13.920 --> 23:18.200] 其他 M4 iPad Pro 的机身表面温度明显比 M2 机型来的更低\n",
            "[23:18.200 --> 23:19.720] 而且看热量分布的情况\n",
            "[23:19.720 --> 23:21.640] 机身散热也的确有所改进\n",
            "[23:21.640 --> 23:23.320] 如果你还嫌这个热的话\n",
            "[23:23.320 --> 23:25.480] 可以把渲染精度开到最低\n",
            "[23:25.480 --> 23:27.800] 这差不多就是手机级别的画质了\n",
            "[23:27.800 --> 23:28.680] 这种情况下\n",
            "[23:28.680 --> 23:33.240] M4 iPad Pro 包括屏幕的整机功耗会降低到 7W 度\n",
            "[23:33.240 --> 23:35.400] 最高温度甚至不到 40 度\n",
            "[23:35.400 --> 23:36.600] 感觉我握着它\n",
            "[23:36.600 --> 23:39.640] 我的手甚至是在给它加热\n",
            "[23:39.680 --> 23:40.640] 这 60 帧下\n",
            "[23:40.640 --> 23:43.960] 原神一根直线的表情实在是太无聊了\n",
            "[23:43.960 --> 23:47.160] 所以我们直接用最高画质解锁了 120 帧\n",
            "[23:47.160 --> 23:48.480] 来看看这个状况下\n",
            "[23:48.480 --> 23:50.320] M4 还能不能撑得住\n",
            "[23:50.320 --> 23:51.080] 事实证明\n",
            "[23:51.080 --> 23:54.000] 这个分辨率下的原神压力还是非常可怕的\n",
            "[23:54.000 --> 23:57.480] 解锁后的 M4 iPad Pro 在 120 帧下只能撑一分钟左右\n",
            "[23:57.480 --> 23:59.560] 然后会掉到 80 帧维持一段时间\n",
            "[23:59.560 --> 24:00.440] 15 分钟之后\n",
            "[24:00.440 --> 24:02.200] 随着机身热量逐渐累积\n",
            "[24:02.200 --> 24:05.280] 帧数也会慢慢掉到 60 到 80 帧之间波动\n",
            "[24:05.280 --> 24:06.040] 事实证明\n",
            "[24:06.040 --> 24:07.600] 就算是 M4\n",
            "[24:07.640 --> 24:10.720] 也很难在超过 2K 的桌面分辨率下\n",
            "[24:10.720 --> 24:12.280] 拗 120 帧的原神\n",
            "[24:12.280 --> 24:14.960] 那咱们再来看看崩坏星穹铁道当中的表现\n",
            "[24:14.960 --> 24:17.000] 星铁的分辨率和原神是一样的\n",
            "[24:17.000 --> 24:19.880] 但是显然星铁的压力还是要更高一些\n",
            "[24:19.880 --> 24:23.360] 这次 M4 的三台 iPad Pro 也并没有扛住摧残\n",
            "[24:23.360 --> 24:24.640] 两台 11 寸的机器\n",
            "[24:24.640 --> 24:27.280] 在测试开始 10 分钟左右就已经开始掉帧了\n",
            "[24:27.280 --> 24:29.200] 13 寸稍微坚持久一点\n",
            "[24:29.200 --> 24:32.680] 但是到了 15 分钟左右后面也还是得降频\n",
            "[24:32.680 --> 24:35.480] 星铁的测试成绩倒是可以非常容易看得出来\n",
            "[24:35.520 --> 24:39.360] M4 的 GPU 在实际游戏当中相较于 M2 的提升幅度\n",
            "[24:39.360 --> 24:40.320] 你可以看得出\n",
            "[24:40.320 --> 24:41.920] 在整机功耗接近的情况下\n",
            "[24:41.920 --> 24:46.240] M4 的 iPad Pro 平均帧数比起 M2 基本高了 50% 左右\n",
            "[24:46.240 --> 24:47.440] 还是非常强的\n",
            "[24:47.440 --> 24:49.840] 我也尝试了一下星铁解锁 120 帧的表现\n",
            "[24:49.840 --> 24:50.640] 不愧是星铁\n",
            "[24:50.640 --> 24:52.360] 这压力实在是大的离谱\n",
            "[24:52.360 --> 24:54.320] 120 帧下也就撑了几秒钟\n",
            "[24:54.320 --> 24:56.320] 然后就迅速掉到了 70 到 80 帧\n",
            "[24:56.320 --> 24:57.760] 接着在机身热起来之后\n",
            "[24:57.760 --> 24:59.400] 后半程也维持不住 60 帧\n",
            "[24:59.400 --> 25:01.520] 这压力还是太难为 M4了\n",
            "[25:01.520 --> 25:03.280] 那么除了这两个手游以外\n",
            "[25:03.280 --> 25:07.080] 不是已经有一些 PC 移植的 3A 游戏上架 iPad 平台了吗?\n",
            "[25:07.080 --> 25:08.880] 那咱们要不要也测测看呢?\n",
            "[25:08.880 --> 25:09.560] 很遗憾\n",
            "[25:09.560 --> 25:11.880] 不管是生化危机 4 死亡搁浅\n",
            "[25:11.880 --> 25:13.000] 还是 COD 战区\n",
            "[25:13.000 --> 25:16.680] 这些 3A 游戏还并没有对 M4 iPad Pro 进行适配\n",
            "[25:16.680 --> 25:18.640] 目前的画质都非常的糊\n",
            "[25:18.640 --> 25:20.880] 唯一能跑的也就是生化危机 8 了\n",
            "[25:20.880 --> 25:23.840] 因为生化危机 8 可以非常方便地改到最高画质\n",
            "[25:23.840 --> 25:24.760] 原生分辨率\n",
            "[25:24.760 --> 25:25.640] 其他几个游戏\n",
            "[25:25.640 --> 25:26.880] 你就算去改文件\n",
            "[25:26.880 --> 25:28.040] 其实也不生效\n",
            "[25:28.040 --> 25:30.440] 即便是拉满了画质的生化危机 8\n",
            "[25:30.480 --> 25:33.280] 我们似乎也遇到了卡功耗强的情况\n",
            "[25:33.280 --> 25:35.360] 明明帧数都没有跑到 60 帧\n",
            "[25:35.360 --> 25:37.600] 功耗却锁在了省机 10W 左右\n",
            "[25:37.600 --> 25:41.000] 可能是游戏开发者已经设定了功耗键诊\n",
            "[25:41.000 --> 25:41.880] 不过这样也好\n",
            "[25:41.880 --> 25:44.200] 可以保证长期不发热玩 3A\n",
            "[25:44.200 --> 25:46.160] 考虑到这个被动散热的机身\n",
            "[25:46.160 --> 25:48.000] 再加上这么低的功耗强\n",
            "[25:48.000 --> 25:50.440] M4 能够在 2K 以上的分辨率上\n",
            "[25:50.440 --> 25:52.080] 跑在接近 50 帧下\n",
            "[25:52.080 --> 25:53.280] 也是挺厉害的\n",
            "[25:53.280 --> 25:55.280] 在 iPad 上玩这些 3A 游戏\n",
            "[25:55.280 --> 25:56.560] 说实话更像主机\n",
            "[25:56.560 --> 25:57.800] 而不是像 PC\n",
            "[25:57.800 --> 25:59.480] 你下载下来就只管玩\n",
            "[25:59.480 --> 26:01.600] 画质选项正常都是不开放的\n",
            "[26:01.600 --> 26:05.040] 这些 3A 游戏反而不太能榨干 iPad 的性能\n",
            "[26:05.040 --> 26:07.280] 这个是测试之前我没有想到的\n",
            "[26:07.280 --> 26:09.560] 那么这次在伦敦的 iPad Pro 发布会上\n",
            "[26:09.560 --> 26:12.720] 苹果很少见的提及了 iPad 的散热表现\n",
            "[26:12.720 --> 26:14.640] 这两天各家的拆机视频也出了\n",
            "[26:14.640 --> 26:15.800] 你们也应该看到了\n",
            "[26:15.800 --> 26:18.200] 这次新 iPad 的确在机身里面\n",
            "[26:18.200 --> 26:19.920] 做了一些散热上的改进\n",
            "[26:19.920 --> 26:22.440] 那么实际的解热能力有没有提升呢?\n",
            "[26:22.440 --> 26:23.840] 那么 11 寸 iPad Pro 这边\n",
            "[26:23.840 --> 26:24.920] M4 的 iPad Pro\n",
            "[26:24.920 --> 26:27.000] 无论是 3 加 6 还是 4 加 6 的型号\n",
            "[26:27.040 --> 26:29.760] 基本上持续的解热能力都在 13W 左右\n",
            "[26:29.760 --> 26:32.840] 比起上代的 10W 左右的确有着显著的进步\n",
            "[26:32.840 --> 26:34.640] 13 寸版本因为机身巨大\n",
            "[26:34.640 --> 26:36.800] 所以散热能力本来也不成问题\n",
            "[26:36.800 --> 26:38.880] 新款的 13 寸 M4 iPad Pro\n",
            "[26:38.880 --> 26:40.320] 有着 14W 的解热能力\n",
            "[26:40.320 --> 26:43.080] 也比起上代的 13.5W 略有提升\n",
            "[26:43.080 --> 26:45.040] 机身表面温度也低了一度多\n",
            "[26:45.040 --> 26:45.920] 总的来说\n",
            "[26:45.920 --> 26:48.400] M4 这台 iPad Pro 确实散热还不错\n",
            "[26:48.400 --> 26:50.040] 并不像 iPhone 这么拉垮\n",
            "[26:50.040 --> 26:50.920] 那么问题来了\n",
            "[26:50.920 --> 26:54.640] 苹果你们什么时候给 iPhone 赶紧升级下散热\n",
            "[26:54.680 --> 26:57.120] 看到竞品一个一个往里面堆均热版\n",
            "[26:57.120 --> 26:58.880] 你们就不着急吗?\n",
            "[26:58.880 --> 27:00.960] 那么最后一个最重要的问题是\n",
            "[27:00.960 --> 27:03.640] M4 的 iPad Pro 续航如何呢?\n",
            "[27:03.640 --> 27:04.240] 要知道\n",
            "[27:04.240 --> 27:06.160] 以往 iPad Pro 的续航\n",
            "[27:06.160 --> 27:08.240] 几乎是用户堵查最多的方面\n",
            "[27:08.240 --> 27:09.800] 所以对 iPad Pro 来说\n",
            "[27:09.800 --> 27:11.920] 最大的考验其实就是续航\n",
            "[27:11.920 --> 27:14.240] 考虑到我们用平板的用场景是比较单一\n",
            "[27:14.240 --> 27:15.760] 且持续时间较长的\n",
            "[27:15.760 --> 27:17.680] 所以这次 iPad 的续航测试\n",
            "[27:17.680 --> 27:19.640] 我们是每两小时一个循环\n",
            "[27:19.640 --> 27:21.640] 包括浏览 20 分钟 IT 之家\n",
            "[27:21.640 --> 27:22.920] 打原神 40 分钟\n",
            "[27:22.960 --> 27:25.160] 以及看 B 站视频 1 小时\n",
            "[27:25.160 --> 27:27.520] 从满电一直循环测到没电\n",
            "[27:27.520 --> 27:30.120] 屏幕亮度则维持在 300 nits\n",
            "[27:30.120 --> 27:32.000] 那么测试结果非常令人意外\n",
            "[27:32.000 --> 27:35.440] M4 的 iPad Pro 续航比起上代有了巨大的提升\n",
            "[27:35.440 --> 27:36.840] 尤其是 13 寸版本\n",
            "[27:36.840 --> 27:41.280] 续航成绩相比 M2 的 iPad Pro 几乎得到了翻倍\n",
            "[27:41.280 --> 27:42.960] 11 寸的版本也有不小的提升\n",
            "[27:42.960 --> 27:44.760] 至于 M2 的 iPad Air\n",
            "[27:44.760 --> 27:48.720] 它的续航则介于 M4 的 iPad Pro 和 M2 的 iPad Pro 之间\n",
            "[27:48.720 --> 27:52.120] 这代 iPad Pro 的续航完全是出乎我意料的\n",
            "[27:52.160 --> 27:55.760] 因为苹果官网标定的续航甚至并没有什么变化\n",
            "[27:55.760 --> 27:59.200] 我们实际的应用测试当中怎么会强这么多呢\n",
            "[27:59.200 --> 28:02.320] 它们芯片之间的能耗差距也没有这么大\n",
            "[28:02.320 --> 28:03.520] 其实对于平板来说\n",
            "[28:03.520 --> 28:05.840] 影响续航的除了芯片的能效以外\n",
            "[28:05.840 --> 28:07.600] 屏幕功耗才是大头\n",
            "[28:07.600 --> 28:09.080] 我们来凑一个很简单的测试\n",
            "[28:09.080 --> 28:10.680] 在 300 nits 的屏幕亮度下\n",
            "[28:10.680 --> 28:11.880] 播放 B 站视频\n",
            "[28:11.880 --> 28:13.360] 测量一下平均功耗\n",
            "[28:13.360 --> 28:14.360] 结果很明显\n",
            "[28:14.360 --> 28:17.120] 上代 M2 12.9 寸 iPad Pro\n",
            "[28:17.120 --> 28:20.080] 看视频的功耗可以说是鹤立鸡群的高\n",
            "[28:20.120 --> 28:22.480] 这就是 miniLED 存在的最大问题\n",
            "[28:22.480 --> 28:24.760] 它的功耗实在是太高了\n",
            "[28:24.760 --> 28:27.000] 而且由于它的背光是动态调整的\n",
            "[28:27.000 --> 28:29.040] 如果你经常浏览网页或者打字\n",
            "[28:29.040 --> 28:30.640] 白色显示区域比较多的话\n",
            "[28:30.640 --> 28:32.240] 它的耗电量只会更高\n",
            "[28:32.240 --> 28:36.240] 这代 OLED 屏幕的确是显著降低了整机功耗\n",
            "[28:36.240 --> 28:38.000] 再配合上芯片的能效进步\n",
            "[28:38.000 --> 28:42.720] 最终就让 M4 的 iPad Pro 在续航方面有了巨大的改善\n",
            "[28:42.720 --> 28:44.320] 做完 M4 的评测\n",
            "[28:44.320 --> 28:45.840] 我最大的感慨就是\n",
            "[28:45.840 --> 28:47.520] 苹果的确努力了\n",
            "[28:47.560 --> 28:51.560] 但似乎我们人类的半导体技术确实要往倒头了\n",
            "[28:51.560 --> 28:53.040] 还记得 7nm 时代的时候\n",
            "[28:53.040 --> 28:56.680] A12、A12X、865、Z2、Z3\n",
            "[28:56.680 --> 29:00.040] 个个都比起自己的前代产品有着巨大的提升\n",
            "[29:00.040 --> 29:03.280] 5nm 时代的 A15、8-2、4090\n",
            "[29:03.280 --> 29:05.360] 也给我们留下了深刻的印象\n",
            "[29:05.360 --> 29:08.840] 但当台积电的工艺制程终于进化到了 3nm 节点\n",
            "[29:08.840 --> 29:11.080] 似乎连着两代的制程\n",
            "[29:11.080 --> 29:13.280] 留给我们的都只有失望\n",
            "[29:13.280 --> 29:14.960] 作为第一颗 N3E 的处理器\n",
            "[29:14.960 --> 29:17.320] M4 拥有业界最先进的处理器架构\n",
            "[29:17.320 --> 29:20.960] 只隔了半年时间居然再次优化了 M3 上的新架构\n",
            "[29:20.960 --> 29:24.640] N3E 也的确让新处理器获得了更高的频率上限\n",
            "[29:24.640 --> 29:25.960] 但这是有代价的\n",
            "[29:25.960 --> 29:28.600] 你会看到 M4 在追求极限性能的路上\n",
            "[29:28.600 --> 29:30.600] 似乎带上了 Intel 的影子\n",
            "[29:30.600 --> 29:31.800] 还记得当年的酷睿2\n",
            "[29:31.800 --> 29:35.400] 正是以一记绝尘的能效优势席卷了那个时代\n",
            "[29:35.400 --> 29:36.240] 几年前\n",
            "[29:36.240 --> 29:39.080] 苹果用 M1 的能效笔征服了轻薄本\n",
            "[29:39.080 --> 29:40.920] 改变了 PC 行业的思路\n",
            "[29:40.920 --> 29:42.480] 但随着时间的推移\n",
            "[29:42.480 --> 29:44.400] 曾经能耗笔优秀的酷睿2\n",
            "[29:44.400 --> 29:46.560] 最终却进化成了如今的大火炉\n",
            "[29:46.560 --> 29:48.520] 酷睿 i9-14900K\n",
            "[29:48.520 --> 29:50.320] 那苹果的 M 系列处理器\n",
            "[29:50.320 --> 29:52.200] 未来也会走这样的路吗?\n",
            "[29:52.200 --> 29:53.520] 我无法预知未来\n",
            "[29:53.520 --> 29:54.120] 但我知道\n",
            "[29:54.120 --> 29:58.160] 通过更激进的功耗预算来提升性能的路是有尽头的\n",
            "[29:58.160 --> 29:59.040] 就 M4 而言\n",
            "[29:59.040 --> 30:01.600] 我认为它达到了 Mac 平台的甜点频率\n",
            "[30:01.600 --> 30:05.720] 它应该足以将未来的 MacBook Pro 推向一个非常强的性能级别\n",
            "[30:05.720 --> 30:08.320] 但 M4 的强大对于目前的 iPad Pro 来说\n",
            "[30:08.320 --> 30:10.840] 最大的意义可能只是多用几年罢了\n",
            "[30:10.840 --> 30:13.320] 现在离 WWDC 也已经很近了\n",
            "[30:13.360 --> 30:14.680] 希望在 WWDC 上\n",
            "[30:14.680 --> 30:18.720] 我们能够看到 iPad Pro 真正能够利用上 M4 的性能\n",
            "[30:18.720 --> 30:20.720] 如果只是为了多用几年而已\n",
            "[30:20.720 --> 30:21.680] 那 iPad 的未来\n",
            "[30:21.680 --> 30:25.080] 我觉得并不会因为 M4 的到来而更加光明\n",
            "[30:25.080 --> 30:27.000] 好了 以上就是本期节目的全部内容了\n",
            "[30:27.000 --> 30:29.600] 我们这期节目做得真的非常的不容易\n",
            "[30:29.600 --> 30:30.520] 如果你喜欢我们节目\n",
            "[30:30.520 --> 30:32.200] 别忘了点赞、订阅、转发、转发、点赞\n",
            "[30:32.200 --> 30:34.600] 也千万不要忘记关注我们的频道\n",
            "[30:34.600 --> 30:37.640] 我们马上四百万粉丝也会有抽奖福利带给大家\n",
            "[30:37.640 --> 30:39.720] 那我们下一期节目\n",
            "[30:39.720 --> 30:40.240] 再见了\n",
            "Generated srt and txt file in the Google Drive -> Whisper -> result Folder\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(model_user)\n",
        "\n",
        "print(\"Whisper model loaded.\")\n",
        "\n",
        "\n",
        "from whisper.utils import get_writer\n",
        "\n",
        "filename = \"audio.mp3\"\n",
        "input_directory = \"content\"\n",
        "input_file = f\"{input_directory}/{filename}\"\n",
        "if language.lower() == 'auto':\n",
        "  result = model.transcribe(input_file, verbose=True, task=task)\n",
        "else:\n",
        "  result = model.transcribe(input_file, verbose=True, task=task, language=language)\n",
        "\n",
        "if disable_google_drive.lower() == 'no':\n",
        "\n",
        "  transcription_root = \"/content/gdrive/MyDrive/Whisper/result\"\n",
        "\n",
        "else:\n",
        "  os.mkdir(\"result\")\n",
        "  transcription_root = \"/content/result\"\n",
        "\n",
        "# Setup Options for SRT/TXT file\n",
        "options = {\n",
        "    'max_line_width': None,\n",
        "    'max_line_count': None,\n",
        "    'highlight_words': False\n",
        "}\n",
        "\n",
        "# Save as an SRT file\n",
        "srt_writer = get_writer(\"srt\", transcription_root,)\n",
        "srt_writer(result, new_filename, options)\n",
        "\n",
        "if Generate_Plain_Document.lower() == \"yes\":\n",
        "\n",
        "  # Save as TXT file\n",
        "  srt_writer = get_writer(\"txt\", transcription_root,)\n",
        "  srt_writer(result, new_filename2, options)\n",
        "  print(\"Generated srt and txt file in the Google Drive -> Whisper -> result Folder\")\n",
        "\n",
        "else:\n",
        "\n",
        "  print(\"Generated srt file in the Google Drive -> Whisper -> result Folder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 壓縮檔案並上傳\n",
        "如果雲端硬碟被停用了"
      ],
      "metadata": {
        "id": "7rWCkivc2Jhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os.path\n",
        "import requests\n",
        "import pycurl\n",
        "from io import BytesIO\n",
        "\n",
        "if disable_google_drive.lower() == 'yes':\n",
        "    # Creating the ZIP file\n",
        "    filenametime = datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
        "    archived = shutil.make_archive(filenametime, 'zip', '/content/result')\n",
        "\n",
        "    if os.path.exists(archived):\n",
        "        print(archived)\n",
        "\n",
        "        if shutdown_after_complete.lower() == \"yes\":\n",
        "          print(\"Because you can't use direct download with shutdown after complete. So we decided to switch to bashupload insead.\")\n",
        "          file_path = archived\n",
        "          upload_url = 'bashupload.com'\n",
        "          new_new_filename = f'{archived}.zip'\n",
        "\n",
        "          # Initialize a cURL object\n",
        "          c  = pycurl.Curl()\n",
        "\n",
        "          # Prepare the form data\n",
        "          c.setopt(c.URL, upload_url)\n",
        "          c.setopt(c.HTTPPOST, [\n",
        "              ('file', (\n",
        "                # Set the filename in the form data\n",
        "                  c.FORM_FILE, file_path,\n",
        "                  c.FORM_FILENAME, new_new_filename\n",
        "                )),\n",
        "            ])\n",
        "\n",
        "          # Capture the response\n",
        "          response = BytesIO()\n",
        "          c.setopt(c.WRITEFUNCTION, response.write)\n",
        "\n",
        "          # Perform the file upload\n",
        "          c.perform()\n",
        "          data = response.getvalue().decode(\"UTF-8\")\n",
        "          print(data)\n",
        "\n",
        "        elif download_method.lower() == \"bashupload\":\n",
        "          file_path = archived\n",
        "          upload_url = 'bashupload.com'\n",
        "          new_new_filename = f'{new_filename}.zip'\n",
        "\n",
        "          # Initialize a cURL object\n",
        "          c  = pycurl.Curl()\n",
        "\n",
        "          # Prepare the form data\n",
        "          c.setopt(c.URL, upload_url)\n",
        "          c.setopt(c.HTTPPOST, [\n",
        "              ('file', (\n",
        "                # Set the filename in the form data\n",
        "                  c.FORM_FILE, file_path,\n",
        "                  c.FORM_FILENAME, new_new_filename\n",
        "                )),\n",
        "            ])\n",
        "\n",
        "          # Capture the response\n",
        "          response = BytesIO()\n",
        "          c.setopt(c.WRITEFUNCTION, response.write)\n",
        "\n",
        "          # Perform the file upload\n",
        "          c.perform()\n",
        "          data = response.getvalue().decode(\"UTF-8\")\n",
        "          print(data)\n",
        "\n",
        "        else:\n",
        "            files.download(archived)\n",
        "            print(\"File downloaded successfully.\")\n",
        "\n",
        "    else:\n",
        "        print(\"ZIP file not created\")"
      ],
      "metadata": {
        "id": "H-PkcMx22JTF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM61x2JuYBl-"
      },
      "source": [
        "# 自動關閉執行階段"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUKHF0-rYGvq"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "end_time = datetime.now()\n",
        "print('\\u751f\\u7522\\u7e3d\\u6642\\u9577: {}'.format(end_time - start_time))\n",
        "time.sleep (2)\n",
        "\n",
        "if shutdown_after_complete.lower() == \"yes\":\n",
        "    print(\"Shutting Down the Runtime Because Shutdown After Complete is on\")\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HiME-JOSUfhd"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}